{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SMIT3.0\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from deep_translator import GoogleTranslator\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "from pythainlp import sent_tokenize, word_tokenize, correct, spell, Tokenizer\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from pythainlp.util import dict_trie, Trie \n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Custom_Dict = pd.read_csv('./SMIT_Data/DataForModel/Raw_Dictionary.csv', encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "DictCorrect = list(set(DictCorrect))\n",
    "\n",
    "modelPath = \"./Model/SentenceTransformer\"\n",
    "model = SentenceTransformer(modelPath)\n",
    "\n",
    "trie = Trie(DictCorrect)\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "            Server = \"smitazure.database.windows.net\",\n",
    "            Database = \"SafetyAudit\",\n",
    "            uid = 'smitadmin',\n",
    "            pwd = 'Abc12345',\n",
    "            Trusted_Connection = 'no') \n",
    "\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#                             Server = \"TONY\",\n",
    "#                             Database = \"SMIT3\",\n",
    "#                             uid = 'Local_SMIT3.0',\n",
    "#                             pwd = 'Tony123456',\n",
    "#                             Trusted_Connection = 'yes')  \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "GetOldID = pd.read_sql('''\n",
    "                        SELECT MAX(CAST([ID] AS int))\n",
    "                        FROM [dbo].[LOG_Finding]\n",
    "                        WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "\t                        (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') \n",
    "\t                        AND Corrective != '';''', connect_db)\n",
    "\n",
    "GetOldID = GetOldID[''].tolist()[0]\n",
    "\n",
    "# Query = '''SELECT [Area], [AuditResult], [Finding], [Corrective] FROM [LOG_Finding] \n",
    "#             WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND \n",
    "#                 ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "#                 (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') AND \n",
    "#                 Corrective != '' AND \n",
    "#                 ID > '''\n",
    "\n",
    "\n",
    "# tbFinding = pd.read_sql(Query+str(GetOldID), connect_db)\n",
    "\n",
    "Query = '''SELECT [Area], [AuditResult], [Finding], [Corrective] FROM [LOG_Finding] \n",
    "            WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND \n",
    "                ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "                (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') AND \n",
    "                Corrective != '' '''\n",
    "\n",
    "tbFinding = pd.read_sql(Query, connect_db)\n",
    "\n",
    "\n",
    "SafetyAudit = pd.read_csv(\"./SMIT_Data/AllRawSafetyAudit.csv\", encoding='utf-8')\n",
    "\n",
    "# # tbFinding = pd.read_csv('./SMIT_Data/Classification_tbFinding/tbFinding.csv', encoding='utf-8')[TotalOldClassification_Finding:]\n",
    "\n",
    "Finding = tbFinding['Finding'].tolist()\n",
    "Translate_tbFinding = []\n",
    "\n",
    "for sentence in Finding:\n",
    "    Translate_tbFinding.append(GoogleTranslator(source = 'auto', target = 'en').translate(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Type of Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepared Cluster : Unsafe Action ------------------\n",
    "\n",
    "ENData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Action\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Action\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeAction = ' '.join(ENData_UnsafeAction+THData_UnsafeAction)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english') \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeAction]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeAction = list(set(word_tokenize(Data_UnsafeAction, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Unsafe Condition ------------------\n",
    "\n",
    "ENData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Condition\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Condition\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeCondition = ' '.join(ENData_UnsafeCondition+THData_UnsafeCondition)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeCondition]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeCondition = list(set(word_tokenize(Data_UnsafeCondition, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Near Miss ------------------\n",
    "\n",
    "ENData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Near Miss\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Near Miss\", \"CleansingDetails\"]).tolist()\n",
    "Data_NearMiss = ' '.join(ENData_NearMiss+THData_NearMiss)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_NearMiss]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_NearMiss = list(set(word_tokenize(Data_NearMiss, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : HNM ------------------\n",
    "\n",
    "ENData_HNM = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"HNM\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_HNM = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"HNM\", \"CleansingDetails\"]).tolist()\n",
    "Data_HNM = ' '.join(ENData_HNM+THData_HNM)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HNM]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_HNM = list(set(word_tokenize(Data_HNM, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Accident ------------------\n",
    "ENData_Accident = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Accident\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Accident = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Accident\", \"CleansingDetails\"]).tolist() \n",
    "Data_Accident = ' '.join(ENData_Accident+THData_Accident) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Accident]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Accident = list(set(word_tokenize(Data_Accident, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Encode_CountVector = model.encode(CountVector.get_feature_names_out()) \n",
    "\n",
    "# doc_array = doc_array[0]\n",
    "\n",
    "# Temp_Encode_CountVector = []\n",
    "\n",
    "# for i in range(len(All_Encode_CountVector)):\n",
    "#     temp = []\n",
    "#     temp.append(sum(All_Encode_CountVector[i]))\n",
    "#     temp.append(doc_array[i])\n",
    "\n",
    "#     Temp_Encode_CountVector.append(temp)\n",
    "    \n",
    "# Encode_CountVector = np.array(Temp_Encode_CountVector)\n",
    "\n",
    "# print(All_Encode_CountVector)\n",
    "# y = [1] * 1000\n",
    "# y.extend([2] * 299)\n",
    "\n",
    "# cmap_bold = ListedColormap(['#FF0000'])\n",
    "\n",
    "# clf = NearestCentroid()\n",
    "# clf.fit(Encode_CountVector, y)\n",
    "# y_pred = clf.predict(Encode_CountVector)\n",
    "\n",
    "# x_min, x_max = Encode_CountVector[:, 0].min() - 1, Encode_CountVector[:, 0].max() + 1\n",
    "# y_min, y_max = Encode_CountVector[:, 1].min() - 1, Encode_CountVector[:, 1].max() + 1\n",
    "\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n",
    "\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.scatter(Encode_CountVector[:, 0], Encode_CountVector[:, 1], c=doc_array, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "# plt.title(\"Cluster Unsafe Action\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Cluster in Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean\n",
    "\n",
    "# # Mean_Cluster = mean(list(doc_array))\n",
    "\n",
    "# Cluster_UnsafeAction = [] \n",
    "# Cluster_UnsafeCondition = []\n",
    "# Cluster_NearMiss = []\n",
    "# Cluster_HNM = []\n",
    "# Cluster_Accident = []\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeAction)):\n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeAction[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeCondition)):    \n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeCondition[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_NearMiss)):\n",
    "#     temp.append(ListWords_NearMiss[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_NearMiss.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_HNM)):\n",
    "#     temp.append(ListWords_HNM[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_HNM.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_Accident)):\n",
    "#     temp.append(ListWords_Accident[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_Accident.append(temp)    \n",
    "    \n",
    "# Cluster_UnsafeAction = pd.DataFrame(data = Cluster_UnsafeAction, columns = ['words', 'frequency'])\n",
    "# Cluster_UnsafeCondition = pd.DataFrame(data = Cluster_UnsafeCondition, columns = ['words', 'frequency'])\n",
    "# Cluster_NearMiss = pd.DataFrame(data = Cluster_NearMiss, columns = ['words', 'frequency'])\n",
    "# Cluster_HNM = pd.DataFrame(data = Cluster_HNM, columns = ['words', 'frequency'])\n",
    "# Cluster_Accident = pd.DataFrame(data = Cluster_Accident, columns = ['words', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get TF-IDF Value of Every Cluster\n",
    "\n",
    "# docs = [ListWords_UnsafeAction, ListWords_UnsafeCondition, ListWords_NearMiss, ListWords_HNM, ListWords_Accident]\n",
    "\n",
    "# vocab = set(ListWords_UnsafeAction + ListWords_UnsafeCondition + ListWords_NearMiss + ListWords_HNM + ListWords_Accident)\n",
    "\n",
    "# def tfidf(word, sentence):\n",
    "#     tf = sentence.count(word) / len(sentence)\n",
    "#     idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "#     return round(tf*idf, 4)\n",
    "\n",
    "# TFIDF_UnsafeAction = []\n",
    "# TFIDF_UnsafeCondition = []\n",
    "# TFIDF_NearMiss = []\n",
    "# TFIDF_Hnm = []\n",
    "# TFIDF_Accident = []\n",
    "\n",
    "# for word in vocab:\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeAction))\n",
    "#     TFIDF_UnsafeAction.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeCondition))\n",
    "#     TFIDF_UnsafeCondition.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_NearMiss))\n",
    "#     TFIDF_NearMiss.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_HNM))\n",
    "#     TFIDF_Hnm.append(temp)\n",
    "    \n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_Accident))\n",
    "#     TFIDF_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 6\n",
    "    if word in ListWords_UnsafeAction:\n",
    "        Value -= 1\n",
    "    if word in ListWords_UnsafeCondition:\n",
    "        Value -= 1\n",
    "    if word in ListWords_NearMiss:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HNM:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Accident:\n",
    "        Value -= 1\n",
    "    if Value == 6:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_UnsafeAction = []\n",
    "Cluster_UnsafeCondition = []\n",
    "Cluster_NearMiss = []\n",
    "Cluster_HNM = []\n",
    "Cluster_Accident = []\n",
    "\n",
    "for word in ListWords_UnsafeAction:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "for word in ListWords_UnsafeCondition:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "for word in ListWords_NearMiss:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_NearMiss.append(temp)\n",
    "\n",
    "for word in ListWords_HNM:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HNM.append(temp)\n",
    "\n",
    "for word in ListWords_Accident:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. เปลี่ยนรูปแบบการดึงข้อมูล tbFinding ให้ดึงจาก Database \n",
    "2. เพิ่มการแปลภาษา tbFinding ก่อนดึงข้อมูลมาจาก Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "TotalData = (pd.read_csv('./SMIT_Data/TotalData.csv', encoding='utf-8'))\n",
    "\n",
    "TotalOldClassification_Finding = TotalData['Old'].tolist()[0]\n",
    "TotalLatestClassification_Finding = TotalData['Latest'].tolist()[0]\n",
    "\n",
    "ClassifyTopic = []\n",
    "\n",
    "tbFindingNo = []\n",
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingFinding = tbFinding['Finding'].tolist()\n",
    "\n",
    "for index in range(len(Translate_tbFinding)):\n",
    "    Data = word_tokenize(Translate_tbFinding[index], custom_dict=trie, engine='newmm')\n",
    "    Finding = \"-\"\n",
    "    temp = []\n",
    "    tempmax = 0\n",
    "    max = -1\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeAction])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeAction]):\n",
    "                tempmax += Cluster_UnsafeAction[ListWords_UnsafeAction.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Unsafe Action\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeCondition])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeCondition]):\n",
    "                tempmax += Cluster_UnsafeCondition[ListWords_UnsafeCondition.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Unsafe Condition\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_NearMiss])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_NearMiss]):\n",
    "                tempmax += Cluster_NearMiss[ListWords_NearMiss.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Near Miss\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_HNM])) > 3:\n",
    "        for i in set([x.lower() for x in Data]):\n",
    "            if i in [j[0] for j in Cluster_HNM]:\n",
    "                tempmax += Cluster_HNM[ListWords_HNM.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"HNM\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Accident])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Accident]):\n",
    "                tempmax += Cluster_Accident[ListWords_Accident.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Accident\"\n",
    "    \n",
    "    tbFindingNo.append(str(TotalOldClassification_Finding+index+1)) \n",
    "    tbFindingSubArea.append('Unknown')  \n",
    "    tbFindingContractor.append('Unknown')     \n",
    "    tbFindingTof.append(Finding)\n",
    "    tbFindingTopic.append('Unknown')\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3-Real\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')      \n",
    "\n",
    "\n",
    "Classification_TbFinding = list(zip(tbFindingNo, tbFindingArea, tbFindingSubArea, \n",
    "                                    tbFindingContractor, tbFindingTof, tbFindingTopic, \n",
    "                                    tbFindingFinding, Translate_tbFinding))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Classification_TbFinding] VALUES (?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Classification_TbFinding)\n",
    "\n",
    "connect_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "Old_Size = len(tbFindingFinding)\n",
    "\n",
    "Head = ['Source', 'Old', 'Latest']\n",
    "\n",
    "UpdateSize = [\n",
    "              ['Classfication_TbFinding', TotalOldClassification_Finding, TotalLatestClassification_Finding+Old_Size]\n",
    "             ] \n",
    "\n",
    "with open('./SMIT_Data/TotalData.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(UpdateSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
