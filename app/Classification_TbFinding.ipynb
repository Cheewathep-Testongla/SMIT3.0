{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SMIT3.0\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from deep_translator import GoogleTranslator\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "from pythainlp import sent_tokenize, word_tokenize, correct, spell, Tokenizer\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from pythainlp.util import dict_trie, Trie \n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom_Dict = pd.read_csv('./SMIT_Data/DataForModel/Raw_Dictionary.csv', encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "DictCorrect = list(set(DictCorrect))\n",
    "\n",
    "modelPath = \"./Model/SentenceTransformer\"\n",
    "model = SentenceTransformer(modelPath)\n",
    "\n",
    "trie = Trie(DictCorrect)\n",
    "\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#             Server = \"smitazure.database.windows.net\",\n",
    "#             Database = \"SafetyAudit\",\n",
    "#             uid = 'smitadmin',\n",
    "#             pwd = 'Abc12345',\n",
    "#             Trusted_Connection = 'no') \n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')  \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "GetOldID = pd.read_sql('''\n",
    "                        SELECT MostSimTOF(CAST([ID] AS int))\n",
    "                        FROM [dbo].[LOG_Finding]\n",
    "                        WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "\t                        (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') \n",
    "\t                        AND Corrective != '';''', connect_db)\n",
    "\n",
    "GetOldID = GetOldID[''].tolist()[0]\n",
    "\n",
    "# Query = '''SELECT [Area], [AuditResult], [Finding], [Corrective] FROM [LOG_Finding] \n",
    "#             WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND \n",
    "#                 ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "#                 (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') AND \n",
    "#                 Corrective != '' AND \n",
    "#                 ID > '''\n",
    "\n",
    "\n",
    "# tbFinding = pd.read_sql(Query+str(GetOldID), connect_db)\n",
    "\n",
    "Query = '''SELECT [Area], [AuditResult], [Finding], [Corrective] FROM [LOG_Finding] \n",
    "            WHERE ID > 113 AND ID != 131 AND ID != 5057 AND ID != 5058 AND ID != 190 AND ID != 1483 AND \n",
    "                ID != 1486 AND ID != 1974 AND ID != 132 AND \n",
    "                (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform') AND \n",
    "                Corrective != '' '''\n",
    "\n",
    "tbFinding = pd.read_sql(Query, connect_db)\n",
    "\n",
    "\n",
    "SafetyAudit = pd.read_csv(\"./SMIT_Data/AllRawSafetyAudit_New.csv\", encoding='utf-8')\n",
    "\n",
    "# # tbFinding = pd.read_csv('./SMIT_Data/Classification_tbFinding/tbFinding.csv', encoding='utf-8')[TotalOldClassification_Finding:]\n",
    "\n",
    "Finding = tbFinding['Finding'].tolist()\n",
    "Translate_tbFinding = []\n",
    "\n",
    "for sentence in Finding:\n",
    "    Translate_tbFinding.append(GoogleTranslator(source = 'auto', target = 'en').translate(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753 377 1100 1100\n"
     ]
    },
    {
     "ename": "NotValidPayload",
     "evalue": "12 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotValidPayload\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\SMIT3.0\\app\\Classification_TbFinding.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m Translate_tbFinding \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tbFindingFinding:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     Translate_tbFinding\u001b[39m.\u001b[39mappend(GoogleTranslator(source \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m, target \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mtranslate(sentence))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\deep_translator\\google.py:55\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     50\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    function to translate a text\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m    @param text: desired text to translate\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    @return: str: translated text\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mif\u001b[39;00m is_input_valid(text):\n\u001b[0;32m     56\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_same_source_target() \u001b[39mor\u001b[39;00m is_empty(text):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\deep_translator\\validate.py:18\u001b[0m, in \u001b[0;36mis_input_valid\u001b[1;34m(text, min_chars, max_chars)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mvalidate the target text to translate\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m@param min_chars: min characters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m@return: bool\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m text\u001b[39m.\u001b[39misdigit():\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mraise\u001b[39;00m NotValidPayload(text)\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m min_chars \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(text) \u001b[39m<\u001b[39m max_chars:\n\u001b[0;32m     20\u001b[0m     \u001b[39mraise\u001b[39;00m NotValidLength(text, min_chars, max_chars)\n",
      "\u001b[1;31mNotValidPayload\u001b[0m: 12 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated"
     ]
    }
   ],
   "source": [
    "tbFinding = pd.read_csv(\"./SMIT_Data/Classification_tbFinding/tbFinding.csv\", encoding=\"utf-8\")\n",
    "tbPermit = pd.read_csv(\"./SMIT_Data/Classification_tbFinding/tbPermit.csv\", encoding=\"utf-8\")\n",
    "\n",
    "tbFinding_NonConform = tbFinding[tbFinding[\"AuditResult\"] == \"Non-conform\"]\n",
    "tbFinding_NeedToImprove = tbFinding[tbFinding[\"AuditResult\"] == \"Need to Improve\"]\n",
    "\n",
    "tbFinding = [tbFinding_NonConform, tbFinding_NeedToImprove]\n",
    "tbFinding = pd.concat(tbFinding)\n",
    "\n",
    "UnwantedID = [131, 5057, 5058, 90, 1483, 1486, 1974, 132]\n",
    "tbFinding = tbFinding[tbFinding['ID'] > 113]\n",
    "\n",
    "for i in UnwantedID:\n",
    "    tbFinding = tbFinding[tbFinding['ID'] != i]\n",
    "\n",
    "tbFindingTitle = tbFinding['Title'].tolist()\n",
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingDetail = []\n",
    "tbFindingAuditResult = tbFinding[\"AuditResult\"].tolist()\n",
    "tbFindingFinding = tbFinding[\"Finding\"].tolist()\n",
    "\n",
    "\n",
    "tbPermitTitle = tbPermit['Title'].tolist()\n",
    "tbPermitDetail = tbPermit['Detail'].tolist()\n",
    "\n",
    "for i in tbFindingTitle:\n",
    "    TemptbFindingDetail = tbPermit[tbPermit['Title'] == i]\n",
    "    tbFindingDetail.append(TemptbFindingDetail['Detail'].tolist())\n",
    "\n",
    "print(len(tbFinding_NonConform), len(tbFinding_NeedToImprove), len(tbFinding), len(tbFindingDetail))\n",
    "\n",
    "Custom_Dict = pd.read_csv('./SMIT_Data/DataForModel/Raw_Dictionary.csv', encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "DictCorrect = list(set(DictCorrect))\n",
    "SafetyAudit = pd.read_csv(\"./SMIT_Data/AllRawSafetyAudit_New.csv\", encoding='utf-8')\n",
    "\n",
    "Translate_tbFinding = []\n",
    "\n",
    "for sentence in tbFindingFinding:\n",
    "    # try:\n",
    "    Translate_tbFinding.append(GoogleTranslator(source = 'auto', target = 'en').translate(sentence))\n",
    "    # except:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : TypeOfFinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepared Cluster : Unsafe Action ------------------\n",
    "\n",
    "ENData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Action\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Action\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeAction = ' '.join(ENData_UnsafeAction+THData_UnsafeAction)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english') \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeAction]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeAction = list(set(word_tokenize(Data_UnsafeAction, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Unsafe Condition ------------------\n",
    "\n",
    "ENData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Condition\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Condition\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeCondition = ' '.join(ENData_UnsafeCondition+THData_UnsafeCondition)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeCondition]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeCondition = list(set(word_tokenize(Data_UnsafeCondition, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Near Miss ------------------\n",
    "\n",
    "ENData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Near Miss\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Near Miss\", \"CleansingDetails\"]).tolist()\n",
    "Data_NearMiss = ' '.join(ENData_NearMiss+THData_NearMiss)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_NearMiss]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_NearMiss = list(set(word_tokenize(Data_NearMiss, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : HNM ------------------\n",
    "\n",
    "ENData_HNM = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"HNM\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_HNM = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"HNM\", \"CleansingDetails\"]).tolist()\n",
    "Data_HNM = ' '.join(ENData_HNM+THData_HNM)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HNM]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_HNM = list(set(word_tokenize(Data_HNM, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Accident ------------------\n",
    "ENData_Accident = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Accident\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Accident = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Accident\", \"CleansingDetails\"]).tolist() \n",
    "Data_Accident = ' '.join(ENData_Accident+THData_Accident) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Accident]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Accident = list(set(word_tokenize(Data_Accident, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SafetyAudit = pd.read_csv(\"./SMIT_Data/AllRawSafetyAudit.csv\", encoding='utf-8')\n",
    "\n",
    "# ------------------ Prepared Cluster : LOTO/ LB ------------------\n",
    "\n",
    "ENData_LOTO = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"LOTO/ LB\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_LOTO = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"LOTO/ LB\", \"CleansingDetails\"]).tolist()\n",
    "Data_LOTO = ' '.join(ENData_LOTO+THData_LOTO)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_LOTO]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_LOTO = list(set(word_tokenize(Data_LOTO, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Work at height ------------------\n",
    "\n",
    "ENData_WAH = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work at height\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_WAH = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work at height\", \"CleansingDetails\"]).tolist()\n",
    "Data_WAH = ' '.join(ENData_WAH+THData_WAH)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WAH]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WAH = list(set(word_tokenize(Data_WAH, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Scaffolding ------------------\n",
    "\n",
    "ENData_Scaffolding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Scaffolding\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_Scaffolding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Scaffolding\", \"CleansingDetails\"]).tolist()\n",
    "Data_Scaffolding = ' '.join(ENData_Scaffolding+THData_Scaffolding)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Scaffolding]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_Scaffolding = list(set(word_tokenize(Data_Scaffolding, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Transportation ------------------\n",
    "\n",
    "ENData_Transportation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Transportation\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_Transportation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Transportation\", \"CleansingDetails\"]).tolist()\n",
    "Data_Transportation = ' '.join(ENData_Transportation+THData_Transportation)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Transportation]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_Transportation = list(set(word_tokenize(Data_Transportation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : PTW & JSA ------------------\n",
    "ENData_PTW_JSA = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PTW & JSA\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PTW_JSA = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PTW & JSA\", \"CleansingDetails\"]).tolist() \n",
    "Data_PTW_JSA = ' '.join(ENData_PTW_JSA+THData_PTW_JSA) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PTW_JSA]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_PTW_JSA = list(set(word_tokenize(Data_PTW_JSA, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Process & Operation ------------------\n",
    "ENData_ProcessOperation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Process & Operation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ProcessOperation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Process & Operation\", \"CleansingDetails\"]).tolist() \n",
    "Data_ProcessOperation = ' '.join(ENData_ProcessOperation+THData_ProcessOperation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ProcessOperation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ProcessOperation = list(set(word_tokenize(Data_ProcessOperation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Radiation ------------------\n",
    "ENData_Radiation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Radiation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Radiation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Radiation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Radiation = ' '.join(ENData_Radiation+THData_Radiation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Radiation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Radiation = list(set(word_tokenize(Data_Radiation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Radiation ------------------\n",
    "ENData_Others = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Others\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Others = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Others\", \"CleansingDetails\"]).tolist() \n",
    "Data_Others = ' '.join(ENData_Others+THData_Others) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Others]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Others = list(set(word_tokenize(Data_Others, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Lifting ------------------\n",
    "ENData_Lifting = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Lifting\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Lifting = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Lifting\", \"CleansingDetails\"]).tolist() \n",
    "Data_Lifting = ' '.join(ENData_Lifting+THData_Lifting) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Lifting]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Lifting = list(set(word_tokenize(Data_Lifting, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Housekeeping ------------------\n",
    "ENData_Housekeeping = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Housekeeping\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Housekeeping = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Housekeeping\", \"CleansingDetails\"]).tolist() \n",
    "Data_Housekeeping = ' '.join(ENData_Housekeeping+THData_Housekeeping) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Housekeeping]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Housekeeping = list(set(word_tokenize(Data_Housekeeping, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Tools & Equipment ------------------\n",
    "ENData_ToolsEquipment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Tools & Equipment\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ToolsEquipment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Tools & Equipment\", \"CleansingDetails\"]).tolist() \n",
    "Data_ToolsEquipment = ' '.join(ENData_ToolsEquipment+THData_ToolsEquipment) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ToolsEquipment]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ToolsEquipment = list(set(word_tokenize(Data_ToolsEquipment, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Hot Work ------------------\n",
    "ENData_HotWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Hot Work\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_HotWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Hot Work\", \"CleansingDetails\"]).tolist() \n",
    "Data_HotWork = ' '.join(ENData_HotWork+THData_HotWork) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HotWork]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_HotWork = list(set(word_tokenize(Data_HotWork, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Excavation ------------------\n",
    "ENData_Excavation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Excavation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Excavation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Excavation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Excavation = ' '.join(ENData_Excavation+THData_Excavation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Excavation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Excavation = list(set(word_tokenize(Data_Excavation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : CSE ------------------\n",
    "ENData_CSE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"CSE\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_CSE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"CSE\", \"CleansingDetails\"]).tolist() \n",
    "Data_CSE = ' '.join(ENData_CSE+THData_CSE) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_CSE]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_CSE = list(set(word_tokenize(Data_CSE, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Electrical & Grounding ------------------\n",
    "ENData_ElectricalGrounding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Electrical & Grounding\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ElectricalGrounding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Electrical & Grounding\", \"CleansingDetails\"]).tolist() \n",
    "Data_ElectricalGrounding = ' '.join(ENData_ElectricalGrounding+THData_ElectricalGrounding) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ElectricalGrounding]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ElectricalGrounding = list(set(word_tokenize(Data_ElectricalGrounding, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Paint/ Coat/ Blast ------------------\n",
    "ENData_PaintCoatBlast = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Paint/ Coat/ Blast\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PaintCoatBlast = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Paint/ Coat/ Blast\", \"CleansingDetails\"]).tolist() \n",
    "Data_PaintCoatBlast = ' '.join(ENData_PaintCoatBlast+THData_PaintCoatBlast) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PaintCoatBlast]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PaintCoatBlast = list(set(word_tokenize(Data_PaintCoatBlast, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Chemical Work ------------------\n",
    "ENData_ChemicalWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Chemical Work\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ChemicalWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Chemical Work\", \"CleansingDetails\"]).tolist() \n",
    "Data_ChemicalWork = ' '.join(ENData_ChemicalWork+THData_ChemicalWork) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ChemicalWork]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_ChemicalWork = list(set(word_tokenize(Data_ChemicalWork, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Safety Management ------------------\n",
    "ENData_SafetyManagement = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Safety Management\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_SafetyManagement = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Safety Management\", \"CleansingDetails\"]).tolist() \n",
    "Data_SafetyManagement = ' '.join(ENData_SafetyManagement+THData_SafetyManagement) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_SafetyManagement]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_SafetyManagement = list(set(word_tokenize(Data_SafetyManagement, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : PPE ------------------\n",
    "ENData_PPE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PPE\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PPE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PPE\", \"CleansingDetails\"]).tolist() \n",
    "Data_PPE = ' '.join(ENData_PPE+THData_PPE) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PPE]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PPE = list(set(word_tokenize(Data_PPE, custom_dict = trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Water jet ------------------\n",
    "ENData_WaterJet = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Water jet\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_WaterJet = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Water jet\", \"CleansingDetails\"]).tolist() \n",
    "Data_WaterJet = ' '.join(ENData_WaterJet+THData_WaterJet) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WaterJet]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WaterJet = list(set(word_tokenize(Data_WaterJet, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Pressure test ------------------\n",
    "ENData_PressureTest = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Pressure test\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PressureTest = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Pressure test\", \"CleansingDetails\"]).tolist() \n",
    "Data_PressureTest = ' '.join(ENData_PressureTest+THData_PressureTest) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PressureTest]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PressureTest = list(set(word_tokenize(Data_PressureTest, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : SL Performance ------------------\n",
    "ENData_SLPerformance = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"SL Performance\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_SLPerformance = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"SL Performance\", \"CleansingDetails\"]).tolist() \n",
    "Data_SLPerformance = ' '.join(ENData_SLPerformance+THData_SLPerformance) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_SLPerformance]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_SLPerformance = list(set(word_tokenize(Data_SLPerformance, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Work Procedure ------------------\n",
    "ENData_WorkProcedure = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work Procedure\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_WorkProcedure = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work Procedure\", \"CleansingDetails\"]).tolist() \n",
    "Data_WorkProcedure = ' '.join(ENData_WorkProcedure+THData_WorkProcedure) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WorkProcedure]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WorkProcedure = list(set(word_tokenize(Data_WorkProcedure, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Civil ------------------\n",
    "ENData_Civil = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Civil\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Civil = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Civil\", \"CleansingDetails\"]).tolist() \n",
    "Data_Civil = ' '.join(ENData_Civil+THData_Civil) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Civil]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Civil = list(set(word_tokenize(Data_Civil, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Insulation ------------------\n",
    "ENData_Insulation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Insulation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Insulation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Insulation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Insulation = ' '.join(ENData_Insulation+THData_Insulation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Insulation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Insulation = list(set(word_tokenize(Data_Insulation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Environmental ------------------\n",
    "ENData_Environmental = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Environmental\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Environmental = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Environmental\", \"CleansingDetails\"]).tolist() \n",
    "Data_Environmental = ' '.join(ENData_Environmental+THData_Environmental) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Environmental]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Environmental = list(set(word_tokenize(Data_Environmental, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Installation/ Alignment ------------------\n",
    "ENData_InstallationAlignment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Installation/ Alignment\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_InstallationAlignment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Installation/ Alignment\", \"CleansingDetails\"]).tolist() \n",
    "Data_InstallationAlignment = ' '.join(ENData_InstallationAlignment+THData_InstallationAlignment) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_InstallationAlignment]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_InstallationAlignment = list(set(word_tokenize(Data_InstallationAlignment, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Installation/ Alignment ------------------\n",
    "ENData_Security = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Security\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Security = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Security\", \"CleansingDetails\"]).tolist() \n",
    "Data_Security = ' '.join(ENData_Security+THData_Security) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Security]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Security = list(set(word_tokenize(Data_Security, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Encode_CountVector = model.encode(CountVector.get_feature_names_out()) \n",
    "\n",
    "# doc_array = doc_array[0]\n",
    "\n",
    "# Temp_Encode_CountVector = []\n",
    "\n",
    "# for i in range(len(All_Encode_CountVector)):\n",
    "#     temp = []\n",
    "#     temp.append(sum(All_Encode_CountVector[i]))\n",
    "#     temp.append(doc_array[i])\n",
    "\n",
    "#     Temp_Encode_CountVector.append(temp)\n",
    "    \n",
    "# Encode_CountVector = np.array(Temp_Encode_CountVector)\n",
    "\n",
    "# print(All_Encode_CountVector)\n",
    "# y = [1] * 1000\n",
    "# y.extend([2] * 299)\n",
    "\n",
    "# cmap_bold = ListedColormap(['#FF0000'])\n",
    "\n",
    "# clf = NearestCentroid()\n",
    "# clf.fit(Encode_CountVector, y)\n",
    "# y_pred = clf.predict(Encode_CountVector)\n",
    "\n",
    "# x_min, x_MostSimTOF = Encode_CountVector[:, 0].min() - 1, Encode_CountVector[:, 0].MostSimTOF() + 1\n",
    "# y_min, y_MostSimTOF = Encode_CountVector[:, 1].min() - 1, Encode_CountVector[:, 1].MostSimTOF() + 1\n",
    "\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_MostSimTOF, 0.2), np.arange(y_min, y_MostSimTOF, 0.2))\n",
    "\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.scatter(Encode_CountVector[:, 0], Encode_CountVector[:, 1], c=doc_array, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "# plt.title(\"Cluster Unsafe Action\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Cluster in Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean\n",
    "\n",
    "# # Mean_Cluster = mean(list(doc_array))\n",
    "\n",
    "# Cluster_UnsafeAction = [] \n",
    "# Cluster_UnsafeCondition = []\n",
    "# Cluster_NearMiss = []\n",
    "# Cluster_HNM = []\n",
    "# Cluster_Accident = []\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeAction)):\n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeAction[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeCondition)):    \n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeCondition[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_NearMiss)):\n",
    "#     temp.append(ListWords_NearMiss[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_NearMiss.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_HNM)):\n",
    "#     temp.append(ListWords_HNM[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_HNM.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_Accident)):\n",
    "#     temp.append(ListWords_Accident[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_Accident.append(temp)    \n",
    "    \n",
    "# Cluster_UnsafeAction = pd.DataFrame(data = Cluster_UnsafeAction, columns = ['words', 'frequency'])\n",
    "# Cluster_UnsafeCondition = pd.DataFrame(data = Cluster_UnsafeCondition, columns = ['words', 'frequency'])\n",
    "# Cluster_NearMiss = pd.DataFrame(data = Cluster_NearMiss, columns = ['words', 'frequency'])\n",
    "# Cluster_HNM = pd.DataFrame(data = Cluster_HNM, columns = ['words', 'frequency'])\n",
    "# Cluster_Accident = pd.DataFrame(data = Cluster_Accident, columns = ['words', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get TF-IDF Value of Every Cluster\n",
    "\n",
    "# docs = [ListWords_UnsafeAction, ListWords_UnsafeCondition, ListWords_NearMiss, ListWords_HNM, ListWords_Accident]\n",
    "\n",
    "# vocab = set(ListWords_UnsafeAction + ListWords_UnsafeCondition + ListWords_NearMiss + ListWords_HNM + ListWords_Accident)\n",
    "\n",
    "# def tfidf(word, sentence):\n",
    "#     tf = sentence.count(word) / len(sentence)\n",
    "#     idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "#     return round(tf*idf, 4)\n",
    "\n",
    "# TFIDF_UnsafeAction = []\n",
    "# TFIDF_UnsafeCondition = []\n",
    "# TFIDF_NearMiss = []\n",
    "# TFIDF_Hnm = []\n",
    "# TFIDF_Accident = []\n",
    "\n",
    "# for word in vocab:\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeAction))\n",
    "#     TFIDF_UnsafeAction.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeCondition))\n",
    "#     TFIDF_UnsafeCondition.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_NearMiss))\n",
    "#     TFIDF_NearMiss.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_HNM))\n",
    "#     TFIDF_Hnm.append(temp)\n",
    "    \n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_Accident))\n",
    "#     TFIDF_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Frequency of each cluster of TypeOfFinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 6\n",
    "    if word in ListWords_UnsafeAction:\n",
    "        Value -= 1\n",
    "    if word in ListWords_UnsafeCondition:\n",
    "        Value -= 1\n",
    "    if word in ListWords_NearMiss:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HNM:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Accident:\n",
    "        Value -= 1\n",
    "    if Value == 6:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_UnsafeAction = []\n",
    "Cluster_UnsafeCondition = []\n",
    "Cluster_NearMiss = []\n",
    "Cluster_HNM = []\n",
    "Cluster_Accident = []\n",
    "\n",
    "for word in ListWords_UnsafeAction:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "for word in ListWords_UnsafeCondition:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "for word in ListWords_NearMiss:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_NearMiss.append(temp)\n",
    "\n",
    "for word in ListWords_HNM:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HNM.append(temp)\n",
    "\n",
    "for word in ListWords_Accident:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Frequency of each cluster of topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 29\n",
    "    if word in ListWords_LOTO:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WAH:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Scaffolding:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Transportation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PTW_JSA:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ProcessOperation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Radiation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Others:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Lifting:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Housekeeping:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ToolsEquipment:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HotWork:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Excavation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_CSE:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ElectricalGrounding:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PaintCoatBlast:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ChemicalWork:\n",
    "        Value -= 1\n",
    "    if word in ListWords_SafetyManagement:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PPE:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WaterJet:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PressureTest:\n",
    "        Value -= 1\n",
    "    if word in ListWords_SLPerformance:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WorkProcedure:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Civil:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Insulation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Environmental:\n",
    "        Value -= 1\n",
    "    if word in ListWords_InstallationAlignment:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Security:\n",
    "        Value -= 1\n",
    "    if Value == 28:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_LOTO = []\n",
    "Cluster_WAH = []\n",
    "Cluster_Scaffolding = []\n",
    "Cluster_Transportaion = []\n",
    "Cluster_PTWJSA = []\n",
    "Cluster_ProcessOperation = []\n",
    "Cluster_Radiation = []\n",
    "Cluster_Others = []\n",
    "Cluster_Lifting = []\n",
    "Cluster_Housekeeping = []\n",
    "Cluster_ToolsEquipment = []\n",
    "Cluster_HotWork = []\n",
    "Cluster_Excavation = []\n",
    "Cluster_CSE = []\n",
    "Cluster_ElectricalGrounding = []\n",
    "Cluster_PaintCoatBlast = []\n",
    "Cluster_ChemicalWork = []\n",
    "Cluster_SafetyManagement = []\n",
    "Cluster_PPE = []\n",
    "Cluster_WaterJet = []\n",
    "Cluster_PressureTest = []\n",
    "Cluster_SLPerformance = []\n",
    "Cluster_WorkProcedure = []\n",
    "Cluster_Civil = []\n",
    "Cluster_Insulation = []\n",
    "Cluster_Environmental = []\n",
    "Cluster_InstallationAlignment = []\n",
    "Cluster_Security = []\n",
    "\n",
    "for word in ListWords_LOTO:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_LOTO.append(temp)\n",
    "\n",
    "for word in ListWords_WAH:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WAH.append(temp)\n",
    "\n",
    "for word in ListWords_Scaffolding:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Scaffolding.append(temp)\n",
    "\n",
    "for word in ListWords_Transportation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Transportaion.append(temp)\n",
    "\n",
    "for word in ListWords_PTW_JSA:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PTWJSA.append(temp)\n",
    "\n",
    "for word in ListWords_ProcessOperation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ProcessOperation.append(temp)\n",
    "\n",
    "for word in ListWords_Others:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Others.append(temp)\n",
    "\n",
    "for word in ListWords_Lifting:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Lifting.append(temp)\n",
    "\n",
    "for word in ListWords_Housekeeping:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Housekeeping.append(temp)\n",
    "\n",
    "for word in ListWords_ToolsEquipment:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ToolsEquipment.append(temp)\n",
    "\n",
    "for word in ListWords_HotWork:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HotWork.append(temp)\n",
    "\n",
    "for word in ListWords_Excavation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Excavation.append(temp)\n",
    "\n",
    "for word in ListWords_CSE:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_CSE.append(temp)\n",
    "\n",
    "for word in ListWords_ElectricalGrounding:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ElectricalGrounding.append(temp)\n",
    "\n",
    "for word in ListWords_PaintCoatBlast:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PaintCoatBlast.append(temp)\n",
    "\n",
    "for word in ListWords_ChemicalWork:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ChemicalWork.append(temp)\n",
    "\n",
    "for word in ListWords_SafetyManagement:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_SafetyManagement.append(temp)\n",
    "\n",
    "for word in ListWords_PPE:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PPE.append(temp)\n",
    "\n",
    "for word in ListWords_WaterJet:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WaterJet.append(temp)\n",
    "\n",
    "for word in ListWords_PressureTest:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PressureTest.append(temp)\n",
    "\n",
    "for word in ListWords_SLPerformance:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_SLPerformance.append(temp)\n",
    "\n",
    "for word in ListWords_WorkProcedure:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WorkProcedure.append(temp)\n",
    "\n",
    "for word in ListWords_Civil:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Civil.append(temp)\n",
    "\n",
    "for word in ListWords_Insulation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Insulation.append(temp)\n",
    "\n",
    "for word in ListWords_Environmental:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Environmental.append(temp)\n",
    "\n",
    "for word in ListWords_InstallationAlignment:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_InstallationAlignment.append(temp)\n",
    "\n",
    "for word in ListWords_Security:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Security.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  tbFinding  Database \n",
    "2.  tbFinding  Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Translate_tbFinding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\SMIT3.0\\app\\Classification_TbFinding.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tbFindingTopic \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tbFindingFinding \u001b[39m=\u001b[39m tbFinding[\u001b[39m'\u001b[39m\u001b[39mFinding\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(Translate_tbFinding)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     Data \u001b[39m=\u001b[39m word_tokenize(Translate_tbFinding[index], custom_dict\u001b[39m=\u001b[39mtrie, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewmm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/SMIT3.0/app/Classification_TbFinding.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     Finding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Translate_tbFinding' is not defined"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "TotalData = (pd.read_csv('./SMIT_Data/TotalData.csv', encoding='utf-8'))\n",
    "\n",
    "TotalOldClassification_Finding = TotalData['Old'].tolist()[0]\n",
    "TotalLatestClassification_Finding = TotalData['Latest'].tolist()[0]\n",
    "\n",
    "ClassifyTopic = []\n",
    "\n",
    "tbFindingNo = []\n",
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingFinding = tbFinding['Finding'].tolist()\n",
    "\n",
    "for index in range(len(Translate_tbFinding)):\n",
    "    Data = word_tokenize(Translate_tbFinding[index], custom_dict=trie, engine='newmm')\n",
    "    Finding = \"-\"\n",
    "    Topic = \"-\"\n",
    "    TempMostSimTOF = 0\n",
    "    MostSimTOF = -1\n",
    "\n",
    "    MostSimTopic = -1\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeAction])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeAction]):\n",
    "                tempMostSimTOF += Cluster_UnsafeAction[ListWords_UnsafeAction.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = tempMostSimTOF\n",
    "            Finding = \"Unsafe Action\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeCondition])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeCondition]):\n",
    "                tempMostSimTOF += Cluster_UnsafeCondition[ListWords_UnsafeCondition.index(i)][1]\n",
    "        if tempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = tempMostSimTOF\n",
    "            Finding = \"Unsafe Condition\"\n",
    "            tempMostSimTOF = 0\n",
    " \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_NearMiss])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_NearMiss]):\n",
    "                tempMostSimTOF += Cluster_NearMiss[ListWords_NearMiss.index(i)][1]\n",
    "        if tempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = tempMostSimTOF\n",
    "            Finding = \"Near Miss\"\n",
    "            tempMostSimTOF = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_HNM])) > 3:\n",
    "        for i in set([x.lower() for x in Data]):\n",
    "            if i in [j[0] for j in Cluster_HNM]:\n",
    "                tempMostSimTOF += Cluster_HNM[ListWords_HNM.index(i)][1]\n",
    "        if tempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = tempMostSimTOF\n",
    "            Finding = \"HNM\"\n",
    "            tempMostSimTOF = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Accident])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Accident]):\n",
    "                tempMostSimTOF += Cluster_Accident[ListWords_Accident.index(i)][1]\n",
    "        if tempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = tempMostSimTOF\n",
    "            Finding = \"Accident\"\n",
    "            tempMostSimTOF = 0\n",
    "      \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_LOTO])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_LOTO]):\n",
    "                TempMostSimTopic += Cluster_LOTO[ListWords_LOTO.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"LOTO/ LB\"\n",
    "            TempMostSimTopic = 0  \n",
    "    \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_WAH])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_WAH]):\n",
    "                TempMostSimTopic += Cluster_WAH[ListWords_WAH.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Work at height\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Scaffolding])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Scaffolding]):\n",
    "                TempMostSimTopic += Cluster_Scaffolding[ListWords_Scaffolding.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Scaffolding\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Transportaion])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Transportaion]):\n",
    "                TempMostSimTopic += Cluster_Transportaion[ListWords_Transportation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Transportation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_PTWJSA])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_PTWJSA]):\n",
    "                TempMostSimTopic += Cluster_PTWJSA[ListWords_PTW_JSA.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"PTW & JSA\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_ProcessOperation])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_ProcessOperation]):\n",
    "                TempMostSimTopic += Cluster_ProcessOperation[ListWords_ProcessOperation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Process & Operation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Radiation])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Radiation]):\n",
    "                TempMostSimTopic += Cluster_Radiation[ListWords_Radiation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Radiation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Others])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Others]):\n",
    "                TempMostSimTopic += Cluster_Others[ListWords_Others.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Others\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Lifting])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Lifting]):\n",
    "                TempMostSimTopic += Cluster_Lifting[ListWords_Lifting.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Lifting\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Housekeeping])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Housekeeping]):\n",
    "                TempMostSimTopic += Cluster_Housekeeping[ListWords_Housekeeping.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Housekeeping\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_ToolsEquipment])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_ToolsEquipment]):\n",
    "                TempMostSimTopic += Cluster_ToolsEquipment[ListWords_ToolsEquipment.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Tools & Equipment\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_HotWork])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_HotWork]):\n",
    "                TempMostSimTopic += Cluster_HotWork[ListWords_HotWork.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Hot Work\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Excavation])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Excavation]):\n",
    "                TempMostSimTopic += Cluster_Excavation[ListWords_Excavation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Excavation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_CSE])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_CSE]):\n",
    "                TempMostSimTopic += Cluster_CSE[ListWords_CSE.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"CSE\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_ElectricalGrounding])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_ElectricalGrounding]):\n",
    "                TempMostSimTopic += Cluster_ElectricalGrounding[ListWords_ElectricalGrounding.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Electrical & Grounding\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_PaintCoatBlast])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_PaintCoatBlast]):\n",
    "                TempMostSimTopic += Cluster_PaintCoatBlast[ListWords_PaintCoatBlast.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Paint/ Coat/ Blast\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_ChemicalWork])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_ChemicalWork]):\n",
    "                TempMostSimTopic += Cluster_ChemicalWork[ListWords_ChemicalWork.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Chemical Work\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_SafetyManagement])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_SafetyManagement]):\n",
    "                TempMostSimTopic += Cluster_SafetyManagement[ListWords_SafetyManagement.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Safety Management\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_PPE])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_PPE]):\n",
    "                TempMostSimTopic += Cluster_PPE[ListWords_PPE.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"PPE\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_WaterJet])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_WaterJet]):\n",
    "                TempMostSimTopic += Cluster_WaterJet[ListWords_WaterJet.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Water jet\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_PressureTest])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_PressureTest]):\n",
    "                TempMostSimTopic += Cluster_PressureTest[ListWords_PressureTest.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Pressure test\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_SLPerformance])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_SLPerformance]):\n",
    "                TempMostSimTopic += Cluster_SLPerformance[ListWords_SLPerformance.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"SL Performance\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_WorkProcedure])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_WorkProcedure]):\n",
    "                TempMostSimTopic += Cluster_WorkProcedure[ListWords_WorkProcedure.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Work Procedure\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Civil])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Civil]):\n",
    "                TempMostSimTopic += Cluster_Civil[ListWords_Civil.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Civil\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Insulation])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Insulation]):\n",
    "                TempMostSimTopic += Cluster_Insulation[ListWords_Insulation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Insulation\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Environmental])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Environmental]):\n",
    "                TempMostSimTopic += Cluster_Environmental[ListWords_Environmental.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Environmental\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_InstallationAlignment])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_InstallationAlignment]):\n",
    "                TempMostSimTopic += Cluster_InstallationAlignment[ListWords_InstallationAlignment.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Installation/ Alignment\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Security])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Security]):\n",
    "                TempMostSimTopic += Cluster_Security[ListWords_Security.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Security\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    tbFindingNo.append(str(TotalOldClassification_Finding+index+1)) \n",
    "    tbFindingSubArea.append('Unknown')  \n",
    "    tbFindingContractor.append('Unknown')     \n",
    "    tbFindingTof.append(Finding)\n",
    "    tbFindingTopic.append(Topic)\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3-Real\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')      \n",
    "\n",
    "\n",
    "Classification_TbFinding = list(zip(tbFindingNo, tbFindingArea, tbFindingSubArea, \n",
    "                                    tbFindingContractor, tbFindingTof, tbFindingTopic, \n",
    "                                    tbFindingFinding, Translate_tbFinding))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Classification_TbFinding] VALUES (?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Classification_TbFinding)\n",
    "\n",
    "connect_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "Old_Size = len(tbFindingFinding)\n",
    "\n",
    "Head = ['Source', 'Old', 'Latest']\n",
    "\n",
    "UpdateSize = [\n",
    "              ['Classfication_TbFinding', TotalOldClassification_Finding, TotalLatestClassification_Finding+Old_Size]\n",
    "             ] \n",
    "\n",
    "with open('./SMIT_Data/TotalData.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(UpdateSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
