{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import deepcut\n",
    "from deep_translator import GoogleTranslator\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pythainlp import sent_tokenize, word_tokenize, correct, spell, Tokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from pythainlp.util import dict_trie, Trie \n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')  \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "SafetyAudit = pd.read_sql(\"SELECT * FROM [FindingDetails];\", connect_db)\n",
    "    \n",
    "Custom_Dict = pd.read_csv('../SMIT_Data/DataForModel/Raw_Dictionary.csv',encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "DictCorrect = list(set(DictCorrect))\n",
    "\n",
    "modelPath = \"../Model/SentenceTransformer\"\n",
    "model = SentenceTransformer(modelPath)\n",
    "\n",
    "SafetyAuditDetails = SafetyAudit['Details'].tolist()\n",
    "SafetyAuditCleansingDetails = SafetyAudit['CleansingDetails'].tolist()\n",
    "SafetyAuditTranslateDetails = SafetyAudit['TransCleansingDetails'].tolist()\n",
    "SafetyAuditTopic = SafetyAudit['Topic'].tolist()\n",
    "SafetyAuditTof = SafetyAudit['TypeOfFinding'].tolist()\n",
    "\n",
    "trie = Trie(DictCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbFinding = pd.read_csv('./SMIT_Data/Classification_tbFinding/tbFinding.csv', encoding='utf-8')\n",
    "\n",
    "Finding = tbFinding['Finding'].tolist()\n",
    "Translate_Finding = []\n",
    "\n",
    "\n",
    "for sentence in Finding:\n",
    "    Translate_Finding.append(GoogleTranslator(source = 'auto', target = 'en').translate(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Type of Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepared Cluster : Unsafe Action ------------------\n",
    "\n",
    "ENData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Action\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Action\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeAction = ' '.join(ENData_UnsafeAction+THData_UnsafeAction)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english') \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeAction]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeAction = list(set(word_tokenize(Data_UnsafeAction, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Unsafe Condition ------------------\n",
    "\n",
    "ENData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Condition\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Unsafe Condition\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeCondition = ' '.join(ENData_UnsafeCondition+THData_UnsafeCondition)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeCondition]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeCondition = list(set(word_tokenize(Data_UnsafeCondition, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Near Miss ------------------\n",
    "\n",
    "ENData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Near Miss\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Near Miss\", \"CleansingDetails\"]).tolist()\n",
    "Data_NearMiss = ' '.join(ENData_NearMiss+THData_NearMiss)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_NearMiss]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_NearMiss = list(set(word_tokenize(Data_NearMiss, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : HNM ------------------\n",
    "\n",
    "ENData_HNM = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"HNM\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_HNM = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"HNM\", \"CleansingDetails\"]).tolist()\n",
    "Data_HNM = ' '.join(ENData_HNM+THData_HNM)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HNM]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_HNM = list(set(word_tokenize(Data_HNM, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Accident ------------------\n",
    "ENData_Accident = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Accident\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Accident = (SafetyAudit.loc[SafetyAudit[\"Type of finding\"] == \"Accident\", \"CleansingDetails\"]).tolist() \n",
    "Data_Accident = ' '.join(ENData_Accident+THData_Accident) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Accident]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Accident = list(set(word_tokenize(Data_Accident, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Encode_CountVector = model.encode(CountVector.get_feature_names_out()) \n",
    "\n",
    "# doc_array = doc_array[0]\n",
    "\n",
    "# Temp_Encode_CountVector = []\n",
    "\n",
    "# for i in range(len(All_Encode_CountVector)):\n",
    "#     temp = []\n",
    "#     temp.append(sum(All_Encode_CountVector[i]))\n",
    "#     temp.append(doc_array[i])\n",
    "\n",
    "#     Temp_Encode_CountVector.append(temp)\n",
    "    \n",
    "# Encode_CountVector = np.array(Temp_Encode_CountVector)\n",
    "\n",
    "# print(All_Encode_CountVector)\n",
    "# y = [1] * 1000\n",
    "# y.extend([2] * 299)\n",
    "\n",
    "# cmap_bold = ListedColormap(['#FF0000'])\n",
    "\n",
    "# clf = NearestCentroid()\n",
    "# clf.fit(Encode_CountVector, y)\n",
    "# y_pred = clf.predict(Encode_CountVector)\n",
    "\n",
    "# x_min, x_max = Encode_CountVector[:, 0].min() - 1, Encode_CountVector[:, 0].max() + 1\n",
    "# y_min, y_max = Encode_CountVector[:, 1].min() - 1, Encode_CountVector[:, 1].max() + 1\n",
    "\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n",
    "\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.scatter(Encode_CountVector[:, 0], Encode_CountVector[:, 1], c=doc_array, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "# plt.title(\"Cluster Unsafe Action\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Cluster in Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean\n",
    "\n",
    "# # Mean_Cluster = mean(list(doc_array))\n",
    "\n",
    "# Cluster_UnsafeAction = [] \n",
    "# Cluster_UnsafeCondition = []\n",
    "# Cluster_NearMiss = []\n",
    "# Cluster_HNM = []\n",
    "# Cluster_Accident = []\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeAction)):\n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeAction[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeCondition)):    \n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeCondition[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_NearMiss)):\n",
    "#     temp.append(ListWords_NearMiss[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_NearMiss.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_HNM)):\n",
    "#     temp.append(ListWords_HNM[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_HNM.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_Accident)):\n",
    "#     temp.append(ListWords_Accident[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_Accident.append(temp)    \n",
    "    \n",
    "# Cluster_UnsafeAction = pd.DataFrame(data = Cluster_UnsafeAction, columns = ['words', 'frequency'])\n",
    "# Cluster_UnsafeCondition = pd.DataFrame(data = Cluster_UnsafeCondition, columns = ['words', 'frequency'])\n",
    "# Cluster_NearMiss = pd.DataFrame(data = Cluster_NearMiss, columns = ['words', 'frequency'])\n",
    "# Cluster_HNM = pd.DataFrame(data = Cluster_HNM, columns = ['words', 'frequency'])\n",
    "# Cluster_Accident = pd.DataFrame(data = Cluster_Accident, columns = ['words', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get TF-IDF Value of Every Cluster\n",
    "\n",
    "# docs = [ListWords_UnsafeAction, ListWords_UnsafeCondition, ListWords_NearMiss, ListWords_HNM, ListWords_Accident]\n",
    "\n",
    "# vocab = set(ListWords_UnsafeAction + ListWords_UnsafeCondition + ListWords_NearMiss + ListWords_HNM + ListWords_Accident)\n",
    "\n",
    "# def tfidf(word, sentence):\n",
    "#     tf = sentence.count(word) / len(sentence)\n",
    "#     idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "#     return round(tf*idf, 4)\n",
    "\n",
    "# TFIDF_UnsafeAction = []\n",
    "# TFIDF_UnsafeCondition = []\n",
    "# TFIDF_NearMiss = []\n",
    "# TFIDF_Hnm = []\n",
    "# TFIDF_Accident = []\n",
    "\n",
    "# for word in vocab:\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeAction))\n",
    "#     TFIDF_UnsafeAction.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeCondition))\n",
    "#     TFIDF_UnsafeCondition.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_NearMiss))\n",
    "#     TFIDF_NearMiss.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_HNM))\n",
    "#     TFIDF_Hnm.append(temp)\n",
    "    \n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_Accident))\n",
    "#     TFIDF_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 6\n",
    "    if word in ListWords_UnsafeAction:\n",
    "        Value -= 1\n",
    "    if word in ListWords_UnsafeCondition:\n",
    "        Value -= 1\n",
    "    if word in ListWords_NearMiss:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HNM:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Accident:\n",
    "        Value -= 1\n",
    "    if Value == 6:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_UnsafeAction = []\n",
    "Cluster_UnsafeCondition = []\n",
    "Cluster_NearMiss = []\n",
    "Cluster_HNM = []\n",
    "Cluster_Accident = []\n",
    "\n",
    "for word in ListWords_UnsafeAction:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "for word in ListWords_UnsafeCondition:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "for word in ListWords_NearMiss:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_NearMiss.append(temp)\n",
    "\n",
    "for word in ListWords_HNM:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HNM.append(temp)\n",
    "\n",
    "for word in ListWords_Accident:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. เปลี่ยนรูปแบบการดึงข้อมูล tbFinding ให้ดึงจาก Database \n",
    "2. เพิ่มการแปลภาษา tbFinding ก่อนดึงข้อมูลมาจาก Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "ClassifyTopic = []\n",
    "\n",
    "tbFinding = pd.read_csv('../SMIT_Data/Classification_tbFinding/tbFindingResult.csv', encoding='utf-8')\n",
    "\n",
    "tbFindingNo = []\n",
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingFinding = tbFinding['Finding'].tolist()\n",
    "tbFindingTranslate = tbFinding['TranslateFinding'].tolist()\n",
    "\n",
    "Custom_Dict = pd.read_csv('../SMIT_Data/DataForModel/Raw_Dictionary.csv',encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "trie = Trie(DictCorrect)\n",
    "\n",
    "for index in range(len(tbFindingTranslate)):\n",
    "    Data = word_tokenize(tbFindingTranslate[index], custom_dict=trie, engine='newmm')\n",
    "    Finding = \"-\"\n",
    "    temp = []\n",
    "    tempmax = 0\n",
    "    max = -1\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeAction])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeAction]):\n",
    "                tempmax += Cluster_UnsafeAction[ListWords_UnsafeAction.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Unsafe Action\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_UnsafeCondition])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeCondition]):\n",
    "                tempmax += Cluster_UnsafeCondition[ListWords_UnsafeCondition.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Unsafe Condition\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_NearMiss])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_NearMiss]):\n",
    "                tempmax += Cluster_NearMiss[ListWords_NearMiss.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Near Miss\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_HNM])) > 3:\n",
    "        for i in set([x.lower() for x in Data]):\n",
    "            if i in [j[0] for j in Cluster_HNM]:\n",
    "                tempmax += Cluster_HNM[ListWords_HNM.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"HNM\"\n",
    "\n",
    "    tempmax = 0\n",
    "    if len(set([x.lower() for x in Data]) & set([i[0] for i in Cluster_Accident])) > 3:\n",
    "        for i in (set([x.lower() for x in Data])):\n",
    "            if i in ([j[0] for j in Cluster_Accident]):\n",
    "                tempmax += Cluster_Accident[ListWords_Accident.index(i)][1]\n",
    "        if tempmax > max:\n",
    "            max = tempmax\n",
    "            Finding = \"Accident\"\n",
    "    \n",
    "    tbFindingNo.append(str(index+1)) \n",
    "    tbFindingSubArea.append('Unknown')  \n",
    "    tbFindingContractor.append('Unknown')     \n",
    "    tbFindingTof.append(Finding)\n",
    "    tbFindingTopic.append('Unknown')\n",
    "\n",
    "    temp.append(str(index+1)) \n",
    "    temp.append(tbFindingArea[index])\n",
    "    temp.append('Unknown')\n",
    "    temp.append('Unknown') \n",
    "    temp.append(Finding)\n",
    "    temp.append(tbFindingFinding[index]) \n",
    "\n",
    "    ClassifyTopic.append(temp)\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')      \n",
    "\n",
    "ResponseClassification_TbFinding = pd.read_sql(\"SELECT * FROM Classification_TbFinding ORDER BY FindingNo ASC;\", connect_db)\n",
    "\n",
    "ResponseClassification_TbFinding = list(ResponseClassification_TbFinding.itertuples(index=False, name=None))\n",
    "\n",
    "if(len(ResponseClassification_TbFinding) > 0):\n",
    "    cursor = connect_db.cursor()\n",
    "    Query = \"DELETE FROM [Classification_TbFinding];\"\n",
    "    cursor.execute(Query)\n",
    "    ResponseClassification_TbFinding = []\n",
    "\n",
    "Classification_TbFinding = list(zip(tbFindingNo, tbFindingArea, tbFindingSubArea, \n",
    "                                    tbFindingContractor, tbFindingTof, tbFindingTopic, \n",
    "                                    tbFindingFinding, tbFindingTranslate))\n",
    "                            \n",
    "# print(len(Prepared_Safety_Audit), len(ResponsePreparedFindingDetail))\n",
    "\n",
    "# Prepared_Safety_Audit = list(set(Prepared_Safety_Audit) - set(ResponsePreparedFindingDetail))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Classification_TbFinding] VALUES (?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Classification_TbFinding)\n",
    "connect_db.commit()\n",
    "\n",
    "Head = ['FindingNo', 'Area', 'SubArea', 'Contractor', 'TypeOfFinding', 'Topic', 'Details', 'TranslateDetails']\n",
    "\n",
    "with open('../SMIT_Data/Classification_TbFinding.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(Classification_TbFinding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
