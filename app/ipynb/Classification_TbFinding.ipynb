{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SMIT3.0\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from deep_translator import GoogleTranslator\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "from pythainlp import sent_tokenize, word_tokenize, correct, spell, Tokenizer\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from pythainlp.util import dict_trie, Trie \n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial TotalData with First Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Head = ['Source', 'LatestDate', 'Old', 'Latest']\n",
    "\n",
    "# # Old_Size = len(tbFindingFinding)\n",
    "\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#             Server = \"smitazure.database.windows.net\",\n",
    "#             Database = \"SafetyAudit\",\n",
    "#             uid = 'smitadmin',\n",
    "#             pwd = 'Abc12345',\n",
    "#             Trusted_Connection = 'no') \n",
    "\n",
    "# Query = 'SELECT MIN([dbo].[LOG_Finding].[Created]) FROM [dbo].[LOG_Finding]'\n",
    "\n",
    "# GetLatestDate = pd.read_sql(Query, connect_db)\n",
    "\n",
    "# GetLatestDate = GetLatestDate[\"\"].tolist()[0]\n",
    "\n",
    "# UpdateSize = [\n",
    "#               ['Classfication_TbFinding', GetLatestDate, 0, 0]\n",
    "#              ] \n",
    "\n",
    "# with open('./SMIT_Data/TotalData.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "#   write = csv.writer(f)\n",
    "#   write.writerow(Head)\n",
    "#   write.writerows(UpdateSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Custom_Dict = pd.read_csv('./SMIT_Data/DataForModel/Raw_Dictionary.csv', encoding='utf-8')\n",
    "DictCorrect = Custom_Dict['correct'].tolist()\n",
    "\n",
    "DictCorrect = list(set(DictCorrect))\n",
    "trie = Trie(DictCorrect)\n",
    "\n",
    "TotalData = (pd.read_csv('./SMIT_Data/TotalData.csv', encoding='utf-8'))\n",
    "\n",
    "LatestDate = TotalData['LatestDate'].tolist()[0]\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "            Server = \"smitazure.database.windows.net\",\n",
    "            Database = \"SafetyAudit\",\n",
    "            uid = 'smitadmin',\n",
    "            pwd = 'Abc12345',\n",
    "            Trusted_Connection = 'no') \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "Query = '''\n",
    "            SELECT (CAST([dbo].[LOG_Finding].[ID] AS int)),\n",
    "                  [dbo].[LOG_Finding].[ID],\n",
    "                  [dbo].[LOG_Finding].[Area],\n",
    "            \t[dbo].[LOG_Finding].[Finding],\n",
    "                  [dbo].[LOG_Permit].[Detail],\n",
    "            \t[dbo].[LOG_Finding].[AuditResult]\n",
    "            FROM [dbo].[LOG_Finding]\n",
    "            JOIN [dbo].[LOG_Permit]\n",
    "            ON   [dbo].[LOG_Finding].[Title] = [dbo].[LOG_Permit].[Title]\n",
    "            WHERE [dbo].[LOG_Finding].ID > 113   AND [dbo].[LOG_Finding].ID != 131  AND [dbo].[LOG_Finding].ID != 5057 AND \n",
    "                  [dbo].[LOG_Finding].ID != 5058 AND [dbo].[LOG_Finding].ID != 190  AND [dbo].[LOG_Finding].ID != 1483 AND \n",
    "                  [dbo].[LOG_Finding].ID != 1486 AND [dbo].[LOG_Finding].ID != 1974 AND [dbo].[LOG_Finding].ID != 132  AND \n",
    "                  (AuditResult = 'Need to Improve' or AuditResult = 'Non-conform')  AND [dbo].[LOG_Permit].[Detail] != '' \n",
    "                  AND Corrective != '' AND [dbo].[LOG_Finding].Created >= '''\n",
    "\n",
    "tbFinding = pd.read_sql(Query+\"'\"+str(LatestDate)+\"'\", connect_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n"
     ]
    }
   ],
   "source": [
    "print(len(tbFinding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local DB\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#                             Server = \"TONY\",\n",
    "#                             Database = \"SMIT3\",\n",
    "#                             uid = 'Local_SMIT3.0',\n",
    "#                             pwd = 'Tony123456',\n",
    "#                             Trusted_Connection = 'yes')  \n",
    "\n",
    "# Get tbFinding from csv file\n",
    "# tbFinding = pd.read_csv(\"./SMIT_Data/Classification_tbFinding/tbFinding.csv\", encoding=\"utf-8\")\n",
    "# tbPermit = pd.read_csv(\"./SMIT_Data/Classification_tbFinding/tbPermit.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# tbFinding_NonConform = tbFinding[tbFinding[\"AuditResult\"] == \"Non-conform\"]\n",
    "# tbFinding_NeedToImprove = tbFinding[tbFinding[\"AuditResult\"] == \"Need to Improve\"]\n",
    "\n",
    "# tbFinding = [tbFinding_NonConform, tbFinding_NeedToImprove]\n",
    "# tbFinding = pd.concat(tbFinding)\n",
    "\n",
    "# UnwantedID = [131, 5057, 5058, 90, 1483, 1486, 1974, 132, 395]\n",
    "# tbFinding = tbFinding[tbFinding['ID'] > 113]\n",
    "\n",
    "# for i in UnwantedID:\n",
    "#     tbFinding = tbFinding[tbFinding['ID'] != i]\n",
    "\n",
    "# tbPermit = tbPermit[tbPermit['Detail'].notna()]\n",
    "\n",
    "# tbPermitTitle = tbPermit['Title'].tolist()\n",
    "# tbPermitDetail = tbPermit['Detail'].tolist()\n",
    "\n",
    "# for i in range(len(tbFindingTitle)):\n",
    "#     TemptbFindingDetail = tbPermit[tbPermit['Title'] == tbFindingTitle[i]]\n",
    "#     TemptbFindingDetail = TemptbFindingDetail['Detail'].tolist()\n",
    "#     if(len(TemptbFindingDetail) > 0):\n",
    "#         tbFindingDetail.append(TemptbFindingDetail[0])\n",
    "#     else:\n",
    "#         tbFindingDetail.append(\"-\")\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingDetail = tbFinding['Detail'].tolist()\n",
    "tbFindingTranslateDetail = []\n",
    "tbFindingAuditResult = tbFinding[\"AuditResult\"].tolist()\n",
    "tbFindingFinding = tbFinding[\"Finding\"].tolist()\n",
    "tbFindingTranslateFinding = []\n",
    "\n",
    "IndexError = []\n",
    "\n",
    "for i in range(len(tbFindingFinding)):\n",
    "    try:\n",
    "        tbFindingTranslateFinding.append(GoogleTranslator(source = 'auto', target = 'en').translate(tbFindingFinding[i]))\n",
    "    except:\n",
    "        print(i)\n",
    "        tbFindingArea.pop(i)\n",
    "        tbFindingDetail.pop(i)\n",
    "        tbFindingAuditResult.pop(i)\n",
    "        IndexError.append(i)\n",
    "\n",
    "for i in IndexError:\n",
    "    tbFindingFinding.pop(i)\n",
    "\n",
    "IndexError = []\n",
    "\n",
    "for i in range(len(tbFindingDetail)):\n",
    "    try:\n",
    "        tbFindingTranslateDetail.append(GoogleTranslator(source = 'auto', target = 'en').translate(tbFindingDetail[i]))\n",
    "    except:\n",
    "        print(i)\n",
    "        tbFindingArea.pop(i)\n",
    "        tbFindingFinding.pop(i)\n",
    "        tbFindingAuditResult.pop(i)\n",
    "        IndexError.append(i)\n",
    "\n",
    "for i in IndexError:\n",
    "    tbFindingDetail.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SafetyAudit = pd.read_csv(\"./SMIT_Data/AllRawSafetyAudit_New.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : TypeOfFinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepared Cluster : Unsafe Action ------------------\n",
    "\n",
    "ENData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Action\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeAction = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Action\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeAction = ' '.join(ENData_UnsafeAction+THData_UnsafeAction)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english') \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeAction]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeAction = list(set(word_tokenize(Data_UnsafeAction, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Unsafe Condition ------------------\n",
    "\n",
    "ENData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Condition\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_UnsafeCondition = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Unsafe Condition\", \"CleansingDetails\"]).tolist()\n",
    "Data_UnsafeCondition = ' '.join(ENData_UnsafeCondition+THData_UnsafeCondition)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_UnsafeCondition]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_UnsafeCondition = list(set(word_tokenize(Data_UnsafeCondition, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Near Miss ------------------\n",
    "\n",
    "ENData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Near Miss\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_NearMiss = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Near Miss\", \"CleansingDetails\"]).tolist()\n",
    "Data_NearMiss = ' '.join(ENData_NearMiss+THData_NearMiss)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_NearMiss]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_NearMiss = list(set(word_tokenize(Data_NearMiss, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : HNM ------------------\n",
    "\n",
    "ENData_HNM = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"HNM\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_HNM = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"HNM\", \"CleansingDetails\"]).tolist()\n",
    "Data_HNM = ' '.join(ENData_HNM+THData_HNM)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HNM]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_HNM = list(set(word_tokenize(Data_HNM, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Accident ------------------\n",
    "ENData_Accident = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Accident\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Accident = (SafetyAudit.loc[SafetyAudit[\"TypeOfFinding\"] == \"Accident\", \"CleansingDetails\"]).tolist() \n",
    "Data_Accident = ' '.join(ENData_Accident+THData_Accident) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words='english', vocabulary=DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Accident]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Accident = list(set(word_tokenize(Data_Accident, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dict for 5 Clusters : Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepared Cluster : LOTO/ LB ------------------\n",
    "\n",
    "ENData_LOTO = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"LOTO/ LB\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_LOTO = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"LOTO/ LB\", \"CleansingDetails\"]).tolist()\n",
    "Data_LOTO = ' '.join(ENData_LOTO+THData_LOTO)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_LOTO]).toarray() \n",
    "\n",
    "# ListWords_UnsafeAction = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_LOTO = list(set(word_tokenize(Data_LOTO, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Work at height ------------------\n",
    "\n",
    "ENData_WAH = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work at height\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_WAH = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work at height\", \"CleansingDetails\"]).tolist()\n",
    "Data_WAH = ' '.join(ENData_WAH+THData_WAH)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WAH]).toarray() \n",
    "\n",
    "# ListWords_UnsafeCondition = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WAH = list(set(word_tokenize(Data_WAH, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Scaffolding ------------------\n",
    "\n",
    "ENData_Scaffolding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Scaffolding\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_Scaffolding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Scaffolding\", \"CleansingDetails\"]).tolist()\n",
    "Data_Scaffolding = ' '.join(ENData_Scaffolding+THData_Scaffolding)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Scaffolding]).toarray() \n",
    "\n",
    "# ListWords_NearMiss = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_Scaffolding = list(set(word_tokenize(Data_Scaffolding, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Transportation ------------------\n",
    "\n",
    "ENData_Transportation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Transportation\", \"TransCleansingDetails\"]).tolist()\n",
    "THData_Transportation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Transportation\", \"CleansingDetails\"]).tolist()\n",
    "Data_Transportation = ' '.join(ENData_Transportation+THData_Transportation)\n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Transportation]).toarray() \n",
    "\n",
    "# ListWords_HNM = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_Transportation = list(set(word_tokenize(Data_Transportation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : PTW & JSA ------------------\n",
    "ENData_PTW_JSA = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PTW & JSA\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PTW_JSA = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PTW & JSA\", \"CleansingDetails\"]).tolist() \n",
    "Data_PTW_JSA = ' '.join(ENData_PTW_JSA+THData_PTW_JSA) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PTW_JSA]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_PTW_JSA = list(set(word_tokenize(Data_PTW_JSA, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Process & Operation ------------------\n",
    "ENData_ProcessOperation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Process & Operation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ProcessOperation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Process & Operation\", \"CleansingDetails\"]).tolist() \n",
    "Data_ProcessOperation = ' '.join(ENData_ProcessOperation+THData_ProcessOperation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ProcessOperation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ProcessOperation = list(set(word_tokenize(Data_ProcessOperation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Radiation ------------------\n",
    "ENData_Radiation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Radiation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Radiation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Radiation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Radiation = ' '.join(ENData_Radiation+THData_Radiation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Radiation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Radiation = list(set(word_tokenize(Data_Radiation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Radiation ------------------\n",
    "ENData_Others = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Others\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Others = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Others\", \"CleansingDetails\"]).tolist() \n",
    "Data_Others = ' '.join(ENData_Others+THData_Others) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Others]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Others = list(set(word_tokenize(Data_Others, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Lifting ------------------\n",
    "ENData_Lifting = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Lifting\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Lifting = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Lifting\", \"CleansingDetails\"]).tolist() \n",
    "Data_Lifting = ' '.join(ENData_Lifting+THData_Lifting) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Lifting]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Lifting = list(set(word_tokenize(Data_Lifting, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Housekeeping ------------------\n",
    "ENData_Housekeeping = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Housekeeping\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Housekeeping = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Housekeeping\", \"CleansingDetails\"]).tolist() \n",
    "Data_Housekeeping = ' '.join(ENData_Housekeeping+THData_Housekeeping) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Housekeeping]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Housekeeping = list(set(word_tokenize(Data_Housekeeping, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Tools & Equipment ------------------\n",
    "ENData_ToolsEquipment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Tools & Equipment\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ToolsEquipment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Tools & Equipment\", \"CleansingDetails\"]).tolist() \n",
    "Data_ToolsEquipment = ' '.join(ENData_ToolsEquipment+THData_ToolsEquipment) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ToolsEquipment]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ToolsEquipment = list(set(word_tokenize(Data_ToolsEquipment, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Hot Work ------------------\n",
    "ENData_HotWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Hot Work\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_HotWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Hot Work\", \"CleansingDetails\"]).tolist() \n",
    "Data_HotWork = ' '.join(ENData_HotWork+THData_HotWork) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_HotWork]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_HotWork = list(set(word_tokenize(Data_HotWork, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Excavation ------------------\n",
    "ENData_Excavation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Excavation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Excavation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Excavation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Excavation = ' '.join(ENData_Excavation+THData_Excavation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Excavation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Excavation = list(set(word_tokenize(Data_Excavation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : CSE ------------------\n",
    "ENData_CSE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"CSE\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_CSE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"CSE\", \"CleansingDetails\"]).tolist() \n",
    "Data_CSE = ' '.join(ENData_CSE+THData_CSE) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_CSE]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_CSE = list(set(word_tokenize(Data_CSE, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Electrical & Grounding ------------------\n",
    "ENData_ElectricalGrounding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Electrical & Grounding\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ElectricalGrounding = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Electrical & Grounding\", \"CleansingDetails\"]).tolist() \n",
    "Data_ElectricalGrounding = ' '.join(ENData_ElectricalGrounding+THData_ElectricalGrounding) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ElectricalGrounding]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_ElectricalGrounding = list(set(word_tokenize(Data_ElectricalGrounding, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Paint/ Coat/ Blast ------------------\n",
    "ENData_PaintCoatBlast = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Paint/ Coat/ Blast\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PaintCoatBlast = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Paint/ Coat/ Blast\", \"CleansingDetails\"]).tolist() \n",
    "Data_PaintCoatBlast = ' '.join(ENData_PaintCoatBlast+THData_PaintCoatBlast) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PaintCoatBlast]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PaintCoatBlast = list(set(word_tokenize(Data_PaintCoatBlast, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Chemical Work ------------------\n",
    "ENData_ChemicalWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Chemical Work\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_ChemicalWork = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Chemical Work\", \"CleansingDetails\"]).tolist() \n",
    "Data_ChemicalWork = ' '.join(ENData_ChemicalWork+THData_ChemicalWork) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_ChemicalWork]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_ChemicalWork = list(set(word_tokenize(Data_ChemicalWork, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Safety Management ------------------\n",
    "ENData_SafetyManagement = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Safety Management\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_SafetyManagement = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Safety Management\", \"CleansingDetails\"]).tolist() \n",
    "Data_SafetyManagement = ' '.join(ENData_SafetyManagement+THData_SafetyManagement) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_SafetyManagement]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_SafetyManagement = list(set(word_tokenize(Data_SafetyManagement, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : PPE ------------------\n",
    "ENData_PPE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PPE\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PPE = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"PPE\", \"CleansingDetails\"]).tolist() \n",
    "Data_PPE = ' '.join(ENData_PPE+THData_PPE) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PPE]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PPE = list(set(word_tokenize(Data_PPE, custom_dict = trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Water jet ------------------\n",
    "ENData_WaterJet = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Water jet\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_WaterJet = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Water jet\", \"CleansingDetails\"]).tolist() \n",
    "Data_WaterJet = ' '.join(ENData_WaterJet+THData_WaterJet) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WaterJet]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WaterJet = list(set(word_tokenize(Data_WaterJet, custom_dict=trie, engine='newmm'))) \n",
    "\n",
    "# ------------------ Prepared Cluster : Pressure test ------------------\n",
    "ENData_PressureTest = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Pressure test\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_PressureTest = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Pressure test\", \"CleansingDetails\"]).tolist() \n",
    "Data_PressureTest = ' '.join(ENData_PressureTest+THData_PressureTest) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_PressureTest]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_PressureTest = list(set(word_tokenize(Data_PressureTest, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : SL Performance ------------------\n",
    "ENData_SLPerformance = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"SL Performance\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_SLPerformance = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"SL Performance\", \"CleansingDetails\"]).tolist() \n",
    "Data_SLPerformance = ' '.join(ENData_SLPerformance+THData_SLPerformance) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_SLPerformance]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_SLPerformance = list(set(word_tokenize(Data_SLPerformance, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Work Procedure ------------------\n",
    "ENData_WorkProcedure = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work Procedure\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_WorkProcedure = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Work Procedure\", \"CleansingDetails\"]).tolist() \n",
    "Data_WorkProcedure = ' '.join(ENData_WorkProcedure+THData_WorkProcedure) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_WorkProcedure]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out()))) \n",
    "\n",
    "ListWords_WorkProcedure = list(set(word_tokenize(Data_WorkProcedure, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Civil ------------------\n",
    "ENData_Civil = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Civil\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Civil = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Civil\", \"CleansingDetails\"]).tolist() \n",
    "Data_Civil = ' '.join(ENData_Civil+THData_Civil) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Civil]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Civil = list(set(word_tokenize(Data_Civil, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Insulation ------------------\n",
    "ENData_Insulation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Insulation\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Insulation = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Insulation\", \"CleansingDetails\"]).tolist() \n",
    "Data_Insulation = ' '.join(ENData_Insulation+THData_Insulation) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Insulation]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Insulation = list(set(word_tokenize(Data_Insulation, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Environmental ------------------\n",
    "ENData_Environmental = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Environmental\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Environmental = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Environmental\", \"CleansingDetails\"]).tolist() \n",
    "Data_Environmental = ' '.join(ENData_Environmental+THData_Environmental) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Environmental]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Environmental = list(set(word_tokenize(Data_Environmental, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Installation/ Alignment ------------------\n",
    "ENData_InstallationAlignment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Installation/ Alignment\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_InstallationAlignment = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Installation/ Alignment\", \"CleansingDetails\"]).tolist() \n",
    "Data_InstallationAlignment = ' '.join(ENData_InstallationAlignment+THData_InstallationAlignment) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_InstallationAlignment]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_InstallationAlignment = list(set(word_tokenize(Data_InstallationAlignment, custom_dict=trie, engine='newmm')))\n",
    "\n",
    "# ------------------ Prepared Cluster : Installation/ Alignment ------------------\n",
    "ENData_Security = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Security\", \"TransCleansingDetails\"]).tolist() \n",
    "THData_Security = (SafetyAudit.loc[SafetyAudit[\"Topic\"] == \"Security\", \"CleansingDetails\"]).tolist() \n",
    "Data_Security = ' '.join(ENData_Security+THData_Security) \n",
    "\n",
    "CountVector = CountVectorizer(stop_words = 'english', vocabulary = DictCorrect) \n",
    "doc_array = CountVector.fit_transform([Data_Security]).toarray() \n",
    " \n",
    "# ListWords_Accident = list(set(list(CountVector.get_feature_names_out())))   \n",
    "\n",
    "ListWords_Security = list(set(word_tokenize(Data_Security, custom_dict=trie, engine='newmm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Encode_CountVector = model.encode(CountVector.get_feature_names_out()) \n",
    "\n",
    "# doc_array = doc_array[0]\n",
    "\n",
    "# Temp_Encode_CountVector = []\n",
    "\n",
    "# for i in range(len(All_Encode_CountVector)):\n",
    "#     temp = []\n",
    "#     temp.append(sum(All_Encode_CountVector[i]))\n",
    "#     temp.append(doc_array[i])\n",
    "\n",
    "#     Temp_Encode_CountVector.append(temp)\n",
    "    \n",
    "# Encode_CountVector = np.array(Temp_Encode_CountVector)\n",
    "\n",
    "# print(All_Encode_CountVector)\n",
    "# y = [1] * 1000\n",
    "# y.extend([2] * 299)\n",
    "\n",
    "# cmap_bold = ListedColormap(['#FF0000'])\n",
    "\n",
    "# clf = NearestCentroid()\n",
    "# clf.fit(Encode_CountVector, y)\n",
    "# y_pred = clf.predict(Encode_CountVector)\n",
    "\n",
    "# x_min, x_MostSimTOF = Encode_CountVector[:, 0].min() - 1, Encode_CountVector[:, 0].MostSimTOF() + 1\n",
    "# y_min, y_MostSimTOF = Encode_CountVector[:, 1].min() - 1, Encode_CountVector[:, 1].MostSimTOF() + 1\n",
    "\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_MostSimTOF, 0.2), np.arange(y_min, y_MostSimTOF, 0.2))\n",
    "\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure()\n",
    "\n",
    "# plt.scatter(Encode_CountVector[:, 0], Encode_CountVector[:, 1], c=doc_array, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "# plt.title(\"Cluster Unsafe Action\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Cluster in Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean\n",
    "\n",
    "# # Mean_Cluster = mean(list(doc_array))\n",
    "\n",
    "# Cluster_UnsafeAction = [] \n",
    "# Cluster_UnsafeCondition = []\n",
    "# Cluster_NearMiss = []\n",
    "# Cluster_HNM = []\n",
    "# Cluster_Accident = []\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeAction)):\n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeAction[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_UnsafeCondition)):    \n",
    "#     temp = []\n",
    "#     temp.append(ListWords_UnsafeCondition[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_NearMiss)):\n",
    "#     temp.append(ListWords_NearMiss[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_NearMiss.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_HNM)):\n",
    "#     temp.append(ListWords_HNM[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_HNM.append(temp)\n",
    "\n",
    "# for i in range(len(ListWords_Accident)):\n",
    "#     temp.append(ListWords_Accident[i])\n",
    "#     temp.append(doc_array[i])\n",
    "#     Cluster_Accident.append(temp)    \n",
    "    \n",
    "# Cluster_UnsafeAction = pd.DataFrame(data = Cluster_UnsafeAction, columns = ['words', 'frequency'])\n",
    "# Cluster_UnsafeCondition = pd.DataFrame(data = Cluster_UnsafeCondition, columns = ['words', 'frequency'])\n",
    "# Cluster_NearMiss = pd.DataFrame(data = Cluster_NearMiss, columns = ['words', 'frequency'])\n",
    "# Cluster_HNM = pd.DataFrame(data = Cluster_HNM, columns = ['words', 'frequency'])\n",
    "# Cluster_Accident = pd.DataFrame(data = Cluster_Accident, columns = ['words', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get TF-IDF Value of Every Cluster\n",
    "\n",
    "# docs = [ListWords_UnsafeAction, ListWords_UnsafeCondition, ListWords_NearMiss, ListWords_HNM, ListWords_Accident]\n",
    "\n",
    "# vocab = set(ListWords_UnsafeAction + ListWords_UnsafeCondition + ListWords_NearMiss + ListWords_HNM + ListWords_Accident)\n",
    "\n",
    "# def tfidf(word, sentence):\n",
    "#     tf = sentence.count(word) / len(sentence)\n",
    "#     idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "#     return round(tf*idf, 4)\n",
    "\n",
    "# TFIDF_UnsafeAction = []\n",
    "# TFIDF_UnsafeCondition = []\n",
    "# TFIDF_NearMiss = []\n",
    "# TFIDF_Hnm = []\n",
    "# TFIDF_Accident = []\n",
    "\n",
    "# for word in vocab:\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeAction))\n",
    "#     TFIDF_UnsafeAction.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_UnsafeCondition))\n",
    "#     TFIDF_UnsafeCondition.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_NearMiss))\n",
    "#     TFIDF_NearMiss.append(temp)\n",
    "\n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_HNM))\n",
    "#     TFIDF_Hnm.append(temp)\n",
    "    \n",
    "#     temp = []\n",
    "#     temp.append(word)\n",
    "#     temp.append(tfidf(word, ListWords_Accident))\n",
    "#     TFIDF_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Frequency of each cluster of TypeOfFinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 6\n",
    "    if word in ListWords_UnsafeAction:\n",
    "        Value -= 1\n",
    "    if word in ListWords_UnsafeCondition:\n",
    "        Value -= 1\n",
    "    if word in ListWords_NearMiss:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HNM:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Accident:\n",
    "        Value -= 1\n",
    "    if Value == 6:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_UnsafeAction = []\n",
    "Cluster_UnsafeCondition = []\n",
    "Cluster_NearMiss = []\n",
    "Cluster_HNM = []\n",
    "Cluster_Accident = []\n",
    "\n",
    "for word in ListWords_UnsafeAction:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeAction.append(temp)\n",
    "\n",
    "for word in ListWords_UnsafeCondition:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_UnsafeCondition.append(temp)\n",
    "\n",
    "for word in ListWords_NearMiss:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_NearMiss.append(temp)\n",
    "\n",
    "for word in ListWords_HNM:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HNM.append(temp)\n",
    "\n",
    "for word in ListWords_Accident:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Accident.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Frequency of each cluster of topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFrequency(word):\n",
    "    Value = 29\n",
    "    if word in ListWords_LOTO:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WAH:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Scaffolding:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Transportation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PTW_JSA:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ProcessOperation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Radiation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Others:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Lifting:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Housekeeping:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ToolsEquipment:\n",
    "        Value -= 1\n",
    "    if word in ListWords_HotWork:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Excavation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_CSE:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ElectricalGrounding:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PaintCoatBlast:\n",
    "        Value -= 1\n",
    "    if word in ListWords_ChemicalWork:\n",
    "        Value -= 1\n",
    "    if word in ListWords_SafetyManagement:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PPE:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WaterJet:\n",
    "        Value -= 1\n",
    "    if word in ListWords_PressureTest:\n",
    "        Value -= 1\n",
    "    if word in ListWords_SLPerformance:\n",
    "        Value -= 1\n",
    "    if word in ListWords_WorkProcedure:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Civil:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Insulation:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Environmental:\n",
    "        Value -= 1\n",
    "    if word in ListWords_InstallationAlignment:\n",
    "        Value -= 1\n",
    "    if word in ListWords_Security:\n",
    "        Value -= 1\n",
    "    if Value == 28:\n",
    "        return 0\n",
    "    else:\n",
    "        return Value\n",
    "\n",
    "Cluster_LOTO = []\n",
    "Cluster_WAH = []\n",
    "Cluster_Scaffolding = []\n",
    "Cluster_Transportaion = []\n",
    "Cluster_PTWJSA = []\n",
    "Cluster_ProcessOperation = []\n",
    "Cluster_Radiation = []\n",
    "Cluster_Others = []\n",
    "Cluster_Lifting = []\n",
    "Cluster_Housekeeping = []\n",
    "Cluster_ToolsEquipment = []\n",
    "Cluster_HotWork = []\n",
    "Cluster_Excavation = []\n",
    "Cluster_CSE = []\n",
    "Cluster_ElectricalGrounding = []\n",
    "Cluster_PaintCoatBlast = []\n",
    "Cluster_ChemicalWork = []\n",
    "Cluster_SafetyManagement = []\n",
    "Cluster_PPE = []\n",
    "Cluster_WaterJet = []\n",
    "Cluster_PressureTest = []\n",
    "Cluster_SLPerformance = []\n",
    "Cluster_WorkProcedure = []\n",
    "Cluster_Civil = []\n",
    "Cluster_Insulation = []\n",
    "Cluster_Environmental = []\n",
    "Cluster_InstallationAlignment = []\n",
    "Cluster_Security = []\n",
    "\n",
    "for word in ListWords_LOTO:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_LOTO.append(temp)\n",
    "\n",
    "for word in ListWords_WAH:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WAH.append(temp)\n",
    "\n",
    "for word in ListWords_Scaffolding:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Scaffolding.append(temp)\n",
    "\n",
    "for word in ListWords_Transportation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Transportaion.append(temp)\n",
    "\n",
    "for word in ListWords_PTW_JSA:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PTWJSA.append(temp)\n",
    "\n",
    "for word in ListWords_ProcessOperation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ProcessOperation.append(temp)\n",
    "\n",
    "for word in ListWords_Others:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Others.append(temp)\n",
    "\n",
    "for word in ListWords_Lifting:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Lifting.append(temp)\n",
    "\n",
    "for word in ListWords_Housekeeping:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Housekeeping.append(temp)\n",
    "\n",
    "for word in ListWords_ToolsEquipment:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ToolsEquipment.append(temp)\n",
    "\n",
    "for word in ListWords_HotWork:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_HotWork.append(temp)\n",
    "\n",
    "for word in ListWords_Excavation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Excavation.append(temp)\n",
    "\n",
    "for word in ListWords_CSE:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_CSE.append(temp)\n",
    "\n",
    "for word in ListWords_ElectricalGrounding:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ElectricalGrounding.append(temp)\n",
    "\n",
    "for word in ListWords_PaintCoatBlast:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PaintCoatBlast.append(temp)\n",
    "\n",
    "for word in ListWords_ChemicalWork:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_ChemicalWork.append(temp)\n",
    "\n",
    "for word in ListWords_SafetyManagement:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_SafetyManagement.append(temp)\n",
    "\n",
    "for word in ListWords_PPE:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PPE.append(temp)\n",
    "\n",
    "for word in ListWords_WaterJet:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WaterJet.append(temp)\n",
    "\n",
    "for word in ListWords_PressureTest:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_PressureTest.append(temp)\n",
    "\n",
    "for word in ListWords_SLPerformance:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_SLPerformance.append(temp)\n",
    "\n",
    "for word in ListWords_WorkProcedure:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_WorkProcedure.append(temp)\n",
    "\n",
    "for word in ListWords_Civil:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Civil.append(temp)\n",
    "\n",
    "for word in ListWords_Insulation:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Insulation.append(temp)\n",
    "\n",
    "for word in ListWords_Environmental:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Environmental.append(temp)\n",
    "\n",
    "for word in ListWords_InstallationAlignment:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_InstallationAlignment.append(temp)\n",
    "\n",
    "for word in ListWords_Security:\n",
    "    temp = []\n",
    "    temp.append(word)\n",
    "    temp.append(CreateFrequency(word))\n",
    "    Cluster_Security.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  tbFinding  Database \n",
    "2.  tbFinding  Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "TotalData = (pd.read_csv('./SMIT_Data/TotalData.csv', encoding='utf-8'))\n",
    "\n",
    "TotalOldClassification_Finding = TotalData['Old'].tolist()[0]\n",
    "TotalLatestClassification_Finding = TotalData['Latest'].tolist()[0]\n",
    "\n",
    "ClassifyTopic = []\n",
    "\n",
    "tbFindingNo = []\n",
    "tbFindingArea = tbFinding['Area'].tolist()\n",
    "tbFindingSubArea = []\n",
    "tbFindingContractor = []\n",
    "tbFindingTof = []\n",
    "tbFindingTopic = []\n",
    "tbFindingFinding = tbFinding['Finding'].tolist()\n",
    "\n",
    "for index in range(len(tbFindingArea)):\n",
    "    DataFinding = word_tokenize(tbFindingFinding[index], custom_dict=trie, engine='newmm')\n",
    "    DataDetails = word_tokenize(tbFindingDetail[index], custom_dict=trie, engine='newmm')\n",
    "    Finding = \"-\"\n",
    "    Topic = \"-\"\n",
    "    TempMostSimTOF = 0\n",
    "    TempMostSimTOF = 0\n",
    "    MostSimTopic = -1\n",
    "    MostSimTOF = -1\n",
    "\n",
    "    if len(set([x.lower() for x in DataFinding]) & set([i[0] for i in Cluster_UnsafeAction])) > 3:\n",
    "        for i in (set([x.lower() for x in DataFinding])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeAction]):\n",
    "                TempMostSimTOF += Cluster_UnsafeAction[ListWords_UnsafeAction.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = TempMostSimTOF\n",
    "            Finding = \"Unsafe Action\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataFinding]) & set([i[0] for i in Cluster_UnsafeCondition])) > 3:\n",
    "        for i in (set([x.lower() for x in DataFinding])):\n",
    "            if i in ([j[0] for j in Cluster_UnsafeCondition]):\n",
    "                TempMostSimTOF += Cluster_UnsafeCondition[ListWords_UnsafeCondition.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = TempMostSimTOF\n",
    "            Finding = \"Unsafe Condition\"\n",
    "            TempMostSimTOF = 0\n",
    " \n",
    "    if len(set([x.lower() for x in DataFinding]) & set([i[0] for i in Cluster_NearMiss])) > 3:\n",
    "        for i in (set([x.lower() for x in DataFinding])):\n",
    "            if i in ([j[0] for j in Cluster_NearMiss]):\n",
    "                TempMostSimTOF += Cluster_NearMiss[ListWords_NearMiss.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = TempMostSimTOF\n",
    "            Finding = \"Near Miss\"\n",
    "            TempMostSimTOF = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataFinding]) & set([i[0] for i in Cluster_HNM])) > 3:\n",
    "        for i in set([x.lower() for x in DataFinding]):\n",
    "            if i in [j[0] for j in Cluster_HNM]:\n",
    "                TempMostSimTOF += Cluster_HNM[ListWords_HNM.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = TempMostSimTOF\n",
    "            Finding = \"HNM\"\n",
    "            TempMostSimTOF = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataFinding]) & set([i[0] for i in Cluster_Accident])) > 3:\n",
    "        for i in (set([x.lower() for x in DataFinding])):\n",
    "            if i in ([j[0] for j in Cluster_Accident]):\n",
    "                TempMostSimTOF += Cluster_Accident[ListWords_Accident.index(i)][1]\n",
    "        if TempMostSimTOF > MostSimTOF:\n",
    "            MostSimTOF = TempMostSimTOF\n",
    "            Finding = \"Accident\"\n",
    "            TempMostSimTOF = 0\n",
    "      \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_LOTO])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_LOTO]):\n",
    "                TempMostSimTopic += Cluster_LOTO[ListWords_LOTO.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"LOTO/ LB\"\n",
    "            TempMostSimTopic = 0  \n",
    "    \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_WAH])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_WAH]):\n",
    "                TempMostSimTopic += Cluster_WAH[ListWords_WAH.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Work at height\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Scaffolding])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Scaffolding]):\n",
    "                TempMostSimTopic += Cluster_Scaffolding[ListWords_Scaffolding.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Scaffolding\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Transportaion])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Transportaion]):\n",
    "                TempMostSimTopic += Cluster_Transportaion[ListWords_Transportation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Transportation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_PTWJSA])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_PTWJSA]):\n",
    "                TempMostSimTopic += Cluster_PTWJSA[ListWords_PTW_JSA.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"PTW & JSA\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_ProcessOperation])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_ProcessOperation]):\n",
    "                TempMostSimTopic += Cluster_ProcessOperation[ListWords_ProcessOperation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Process & Operation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Radiation])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Radiation]):\n",
    "                TempMostSimTopic += Cluster_Radiation[ListWords_Radiation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Radiation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Others])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Others]):\n",
    "                TempMostSimTopic += Cluster_Others[ListWords_Others.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Others\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Lifting])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Lifting]):\n",
    "                TempMostSimTopic += Cluster_Lifting[ListWords_Lifting.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Lifting\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Housekeeping])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Housekeeping]):\n",
    "                TempMostSimTopic += Cluster_Housekeeping[ListWords_Housekeeping.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Housekeeping\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_ToolsEquipment])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_ToolsEquipment]):\n",
    "                TempMostSimTopic += Cluster_ToolsEquipment[ListWords_ToolsEquipment.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Tools & Equipment\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_HotWork])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_HotWork]):\n",
    "                TempMostSimTopic += Cluster_HotWork[ListWords_HotWork.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Hot Work\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Excavation])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Excavation]):\n",
    "                TempMostSimTopic += Cluster_Excavation[ListWords_Excavation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Excavation\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_CSE])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_CSE]):\n",
    "                TempMostSimTopic += Cluster_CSE[ListWords_CSE.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"CSE\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_ElectricalGrounding])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_ElectricalGrounding]):\n",
    "                TempMostSimTopic += Cluster_ElectricalGrounding[ListWords_ElectricalGrounding.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Electrical & Grounding\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_PaintCoatBlast])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_PaintCoatBlast]):\n",
    "                TempMostSimTopic += Cluster_PaintCoatBlast[ListWords_PaintCoatBlast.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Paint/ Coat/ Blast\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_ChemicalWork])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_ChemicalWork]):\n",
    "                TempMostSimTopic += Cluster_ChemicalWork[ListWords_ChemicalWork.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Chemical Work\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_SafetyManagement])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_SafetyManagement]):\n",
    "                TempMostSimTopic += Cluster_SafetyManagement[ListWords_SafetyManagement.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Safety Management\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_PPE])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_PPE]):\n",
    "                TempMostSimTopic += Cluster_PPE[ListWords_PPE.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"PPE\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_WaterJet])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_WaterJet]):\n",
    "                TempMostSimTopic += Cluster_WaterJet[ListWords_WaterJet.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Water jet\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_PressureTest])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_PressureTest]):\n",
    "                TempMostSimTopic += Cluster_PressureTest[ListWords_PressureTest.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Pressure test\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_SLPerformance])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_SLPerformance]):\n",
    "                TempMostSimTopic += Cluster_SLPerformance[ListWords_SLPerformance.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"SL Performance\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_WorkProcedure])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_WorkProcedure]):\n",
    "                TempMostSimTopic += Cluster_WorkProcedure[ListWords_WorkProcedure.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Work Procedure\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Civil])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Civil]):\n",
    "                TempMostSimTopic += Cluster_Civil[ListWords_Civil.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Civil\"\n",
    "            TempMostSimTopic = 0\n",
    "            \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Insulation])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Insulation]):\n",
    "                TempMostSimTopic += Cluster_Insulation[ListWords_Insulation.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Insulation\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Environmental])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Environmental]):\n",
    "                TempMostSimTopic += Cluster_Environmental[ListWords_Environmental.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Environmental\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_InstallationAlignment])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_InstallationAlignment]):\n",
    "                TempMostSimTopic += Cluster_InstallationAlignment[ListWords_InstallationAlignment.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Installation/ Alignment\"\n",
    "            TempMostSimTopic = 0\n",
    "                        \n",
    "    if len(set([x.lower() for x in DataDetails]) & set([i[0] for i in Cluster_Security])) > 3:\n",
    "        for i in (set([x.lower() for x in DataDetails])):\n",
    "            if i in ([j[0] for j in Cluster_Security]):\n",
    "                TempMostSimTopic += Cluster_Security[ListWords_Security.index(i)][1]\n",
    "        if TempMostSimTopic > MostSimTopic:\n",
    "            MostSimTopic = TempMostSimTopic\n",
    "            Topic = \"Security\"\n",
    "            TempMostSimTopic = 0\n",
    "\n",
    "    tbFindingNo.append(str(TotalOldClassification_Finding+index+1)) \n",
    "    tbFindingSubArea.append('-')  \n",
    "    tbFindingContractor.append('-')     \n",
    "    tbFindingTof.append(Finding)\n",
    "    tbFindingTopic.append(Topic)\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "            Server = \"smitazure.database.windows.net\",\n",
    "            Database = \"SMIT3\",\n",
    "            uid = 'smitadmin',\n",
    "            pwd = 'Abc12345',\n",
    "            Trusted_Connection = 'no')    \n",
    "\n",
    "# tbFindingDetail = [x.encode('utf-8') for x in tbFindingDetail]\n",
    "# tbFindingFinding = [x.encode('utf-8') for x in tbFindingFinding]\n",
    "\n",
    "Classification_TbFinding = list(zip(tbFindingNo, tbFindingArea, tbFindingSubArea, \n",
    "                                    tbFindingContractor, tbFindingTof, tbFindingTopic, \n",
    "                                    tbFindingDetail, tbFindingTranslateDetail, tbFindingFinding, tbFindingTranslateFinding))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Classification_TbFinding] VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Classification_TbFinding)\n",
    "\n",
    "connect_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:758: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "            Server = \"smitazure.database.windows.net\",\n",
    "            Database = \"SafetyAudit\",\n",
    "            uid = 'smitadmin',\n",
    "            pwd = 'Abc12345',\n",
    "            Trusted_Connection = 'no') \n",
    "\n",
    "Query = 'SELECT MAX([dbo].[LOG_Finding].[Created]) FROM [dbo].[LOG_Finding]'\n",
    "\n",
    "GetLatestDate = pd.read_sql(Query, connect_db)\n",
    "\n",
    "GetLatestDate = GetLatestDate[\"\"].tolist()[0]\n",
    "\n",
    "Head = ['Source', 'LatestDate', 'Old', 'Latest']\n",
    "\n",
    "Old_Size = len(tbFindingFinding)\n",
    "\n",
    "UpdateSize = [\n",
    "              ['Classfication_TbFinding', GetLatestDate, TotalOldClassification_Finding, int(TotalLatestClassification_Finding)+Old_Size],\n",
    "              ['All Record', '-', 0, '-']\n",
    "             ] \n",
    "\n",
    "with open('./SMIT_Data/TotalData.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(UpdateSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
