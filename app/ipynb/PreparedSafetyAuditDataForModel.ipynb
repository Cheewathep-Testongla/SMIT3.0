{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SMIT3.0\n"
     ]
    }
   ],
   "source": [
    "cd SMIT3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เพิ่มการดึง Query ข้อมูลจาก Table AUDITResult, Classification_Finding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator               \n",
    "from app.Function import *                    \n",
    "import pandas as pd                                        \n",
    "import pyodbc                                              \n",
    "import re                                                  \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "modelPath = \"./Model/SentenceTransformer\"\n",
    "\n",
    "model = SentenceTransformer(modelPath)\n",
    " \n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"smitazure.database.windows.net\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'smitadmin',\n",
    "                            pwd = 'Abc12345',\n",
    "                            Trusted_Connection = 'no')   \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "Cleansing_FindingDetails = pd.read_sql(\"SELECT * FROM [Cleansing_FindingDetails]\", connect_db)\n",
    "\n",
    "FindingNo = Cleansing_FindingDetails['FindingNo'].tolist()\n",
    "FindingNo = [\"-\" if pd.isnull(x) else x for x in FindingNo]\n",
    "\n",
    "Area = Cleansing_FindingDetails['Area'].tolist()\n",
    "Area = [\"-\" if pd.isnull(x) else x for x in Area]\n",
    "\n",
    "SubArea = Cleansing_FindingDetails['SubArea'].tolist()\n",
    "SubArea = [\"-\" if pd.isnull(x) else x for x in SubArea]\n",
    "\n",
    "Contractor = Cleansing_FindingDetails['Contractor'].tolist()\n",
    "Contractor = [\"-\" if pd.isnull(x) else x for x in Contractor]\n",
    "\n",
    "Tof = Cleansing_FindingDetails['TypeOfFinding'].tolist()\n",
    "Tof = [\"-\" if pd.isnull(x) else x for x in Tof]\n",
    "\n",
    "Topic = Cleansing_FindingDetails['Topic'].tolist()\n",
    "Topic = [\"-\" if pd.isnull(x) else x for x in Topic]\n",
    "\n",
    "Details = Cleansing_FindingDetails['Finding'].tolist()\n",
    "Details = [\"-\" if pd.isnull(x) else x for x in Details]\n",
    "\n",
    "CleansingDetails = Cleansing_FindingDetails['CleansingFinding'].tolist()\n",
    "CleansingDetails = [\"-\" if pd.isnull(x) else x for x in CleansingDetails]\n",
    "\n",
    "TranslateDetails = Cleansing_FindingDetails['TranslateFinding'].tolist()\n",
    "TranslateDetails = [\"-\" if pd.isnull(x) else x for x in TranslateDetails]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepared_FindingNo = []\n",
    "Prepared_Area = []\n",
    "Prepared_SubArea = []\n",
    "Prepared_Contractor = []\n",
    "Prepared_Tof = []\n",
    "Prepared_Topic = []\n",
    "Prepared_Details = []\n",
    "Prepared_Frequency = []\n",
    "Prepared_CleansingDetails = []\n",
    "Prepared_Translate_Details = []\n",
    "Prepared_ListFindingNo = []\n",
    "MostMatch = []\n",
    "\n",
    "Encode_Translate_Cleansing_Safety_Audit = model.encode(TranslateDetails)\n",
    "\n",
    "Cosine_Sim = util.cos_sim(Encode_Translate_Cleansing_Safety_Audit, Encode_Translate_Cleansing_Safety_Audit)\n",
    "\n",
    "\n",
    "Size = len(Encode_Translate_Cleansing_Safety_Audit)\n",
    "\n",
    "Sum = 0\n",
    "for i in range(0, Size):\n",
    "  Index_Most_Frequency = 0\n",
    "  Count_Frequency = 0\n",
    "  Max_Cosine = 0\n",
    "  Index_Frequency = []\n",
    "  if i not in MostMatch:\n",
    "    if (Tof[i] == \"Unsafe Condition\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.6 and Tof[j] == \"Unsafe Condition\":\n",
    "              Count_Frequency += 1 \n",
    "              Index_Frequency.append(j) \n",
    "              if Cosine_Sim[i][j] > Max_Cosine:\n",
    "                Max_Cosine = Cosine_Sim[i][j]\n",
    "                Index_Most_Frequency = j\n",
    "        \n",
    "    elif (Tof[i] == \"Unsafe Action\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.6 and Tof[j] == \"Unsafe Action\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:\n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"HNM\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"HNM\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine: \n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"Near Miss\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"Near Miss\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:\n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"Accident\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"Accident\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:   \n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "              \n",
    "    if Count_Frequency == 0:\n",
    "      Index_Frequency.append(i)\n",
    "      Count_Frequency = 1 \n",
    "      Index_Most_Frequency = i  \n",
    "\n",
    "    if (Index_Most_Frequency not in MostMatch) and Details[Index_Most_Frequency] not in Prepared_Details:\n",
    "      Prepared_FindingNo.append(int(FindingNo[Index_Most_Frequency]))\n",
    "      Prepared_Area.append(Area[Index_Most_Frequency])\n",
    "      Prepared_SubArea.append(SubArea[Index_Most_Frequency])\n",
    "      Prepared_Contractor.append(Contractor[Index_Most_Frequency])\n",
    "      Prepared_Tof.append(Tof[Index_Most_Frequency])\n",
    "      Prepared_Topic.append(Topic[Index_Most_Frequency])\n",
    "      Prepared_Details.append(Details[Index_Most_Frequency])\n",
    "      Prepared_Frequency.append(Count_Frequency)\n",
    "      Prepared_CleansingDetails.append(CleansingDetails[Index_Most_Frequency])\n",
    "      Prepared_Translate_Details.append(TranslateDetails[Index_Most_Frequency])\n",
    "      Prepared_ListFindingNo.append(Index_Frequency)\n",
    "      MostMatch.append(Index_Most_Frequency)\n",
    "\n",
    "    if Index_Most_Frequency in MostMatch and (Details[Index_Most_Frequency] in Prepared_Details and Area[Index_Most_Frequency] in Prepared_Area):\n",
    "      Index = MostMatch.index(Index_Most_Frequency)\n",
    "      Temp_Prepared_ListFindingNo = Prepared_ListFindingNo[Index]+Index_Frequency\n",
    "      Temp_Prepared_ListFindingNo = list(set(Temp_Prepared_ListFindingNo))\n",
    "      Prepared_ListFindingNo[Index] = Temp_Prepared_ListFindingNo\n",
    "      Prepared_Frequency[Index] = len(Temp_Prepared_ListFindingNo)\n",
    "\n",
    "# print(MostMatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Prepared_FindingNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"smitazure.database.windows.net\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'smitadmin',\n",
    "                            pwd = 'Abc12345',\n",
    "                            Trusted_Connection = 'no')   \n",
    "\n",
    "\n",
    "Prepared_Safety_Audit = list(zip(Prepared_FindingNo, Prepared_Area, Prepared_SubArea, \n",
    "                            Prepared_Contractor, Prepared_Tof, Prepared_Topic, Prepared_Details, \n",
    "                            Prepared_Frequency, Prepared_CleansingDetails, Prepared_Translate_Details))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Prepared_FindingDetails] VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Prepared_Safety_Audit)\n",
    "connect_db.commit()\n",
    "\n",
    "Head = ['FindingNo', 'Area', 'SubArea', 'Contractor', 'TypeOfFinding', 'Topic', 'Finding', 'Frequency', 'CleansingFinding', 'TranslateFinding']\n",
    "\n",
    "with open('./SMIT_Data/Prepared_Safety_Audit.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(Prepared_Safety_Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "modelPath = \"./Model/SentenceTransformer\"\n",
    "# Test\n",
    "model = SentenceTransformer(modelPath)\n",
    "\n",
    "SafetyAudit = pd.read_csv('./SMIT_Data/Prepared_Safety_Audit.csv', encoding='utf-8')\n",
    "SA_Details = SafetyAudit['Finding'].tolist()\n",
    "SA_Area = SafetyAudit['Area'].tolist()\n",
    "SA_Contractor = SafetyAudit['Contractor'].tolist()\n",
    "SA_Tof = SafetyAudit['TypeOfFinding'].tolist()\n",
    "SA_Topic = SafetyAudit['Topic'].tolist()\n",
    "SA_Frequency = SafetyAudit['Frequency'].tolist()\n",
    "SA_Details_Trans = SafetyAudit['TranslateFinding'].tolist()\n",
    "\n",
    "Encode_Safey_Audit_Details = model.encode(SA_Details_Trans)\n",
    "Encode_Safey_Audit_Details = np.array(Encode_Safey_Audit_Details)\n",
    "\n",
    "with open('./SMIT_Data/Encode_SafeyAudit.pkl', 'wb') as files:\n",
    "    pickle.dump(Encode_Safey_Audit_Details, files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
