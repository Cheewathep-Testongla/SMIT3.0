{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\SMIT3.0\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เพิ่มการดึง Query ข้อมูลจาก Table AUDITResult, Classification_Finding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator               \n",
    "from Function import Cleansing_Input                       \n",
    "import pandas as pd                                        \n",
    "import pyodbc                                              \n",
    "import re                                                  \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "modelPath = \"./Model/SentenceTransformer\"\n",
    "\n",
    "model = SentenceTransformer(modelPath)\n",
    "\n",
    "# Real Database Connection\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#                             Server = \"TONY\",\n",
    "#                             Database = \"SMIT3\",\n",
    "#                             uid = 'Local_SMIT3.0',\n",
    "#                             pwd = 'Tony123456',\n",
    "#                             Trusted_Connection = 'yes')  \n",
    "\n",
    "# Local Current Database\n",
    "# connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "#                             Server = \"TONY\",\n",
    "#                             Database = \"SMIT3\",\n",
    "#                             uid = 'Local_SMIT3.0',\n",
    "#                             pwd = 'Tony123456',\n",
    "#                             Trusted_Connection = 'yes')  \n",
    "\n",
    "# Local Used all Data\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3-Real\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')  \n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "Cleansing_FindingDetails = pd.read_sql(\"SELECT * FROM [Cleansing_FindingDetails];\", connect_db)\n",
    "\n",
    "FindingNo = Cleansing_FindingDetails['FindingNo'].tolist()\n",
    "FindingNo = [\"-\" if pd.isnull(x) else x for x in FindingNo]\n",
    "\n",
    "Area = Cleansing_FindingDetails['Area'].tolist()\n",
    "Area = [\"-\" if pd.isnull(x) else x for x in Area]\n",
    "\n",
    "SubArea = Cleansing_FindingDetails['SubArea'].tolist()\n",
    "SubArea = [\"-\" if pd.isnull(x) else x for x in SubArea]\n",
    "\n",
    "Contractor = Cleansing_FindingDetails['Contractor'].tolist()\n",
    "Contractor = [\"-\" if pd.isnull(x) else x for x in Contractor]\n",
    "\n",
    "Tof = Cleansing_FindingDetails['TypeOfFinding'].tolist()\n",
    "Tof = [\"-\" if pd.isnull(x) else x for x in Tof]\n",
    "\n",
    "Topic = Cleansing_FindingDetails['Topic'].tolist()\n",
    "Topic = [\"-\" if pd.isnull(x) else x for x in Topic]\n",
    "\n",
    "Details = Cleansing_FindingDetails['Details'].tolist()\n",
    "Details = [\"-\" if pd.isnull(x) else x for x in Details]\n",
    "\n",
    "CleansingDetails = Cleansing_FindingDetails['CleansingDetails'].tolist()\n",
    "CleansingDetails = [\"-\" if pd.isnull(x) else x for x in CleansingDetails]\n",
    "\n",
    "TranslateDetails = Cleansing_FindingDetails['TranslateDetails'].tolist()\n",
    "TranslateDetails = [\"-\" if pd.isnull(x) else x for x in TranslateDetails]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Data From Excel to Database: FindingDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')  \n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "\n",
    "Query = \"DELETE FROM FindingDetails;\"\n",
    "\n",
    "cursor.execute(Query)\n",
    "\n",
    "TotalData = (pd.read_csv('./SMIT_Data/TotalData.csv', encoding='utf-8'))\n",
    "\n",
    "TotalOldFindingDetails = TotalData['Old'].tolist()[0]\n",
    "\n",
    "FindingDetails = (pd.read_csv('./SMIT_Data/Raw_Safety_Audit.csv', encoding='utf-8'))[TotalOldFindingDetails:]\n",
    "\n",
    "FindingNo = FindingDetails['No.'].tolist()\n",
    "FindingNo = [\"-\" if pd.isnull(x) else x for x in FindingNo]\n",
    "\n",
    "Company = FindingDetails['Company'].tolist()\n",
    "Company = [\"-\" if pd.isnull(x) else x for x in Company]\n",
    "\n",
    "Area = FindingDetails['Area'].tolist()\n",
    "Area = [\"-\" if pd.isnull(x) else x for x in Area]\n",
    "\n",
    "SubArea = FindingDetails['Sub Area'].tolist()\n",
    "SubArea = [\"-\" if pd.isnull(x) else x for x in SubArea]\n",
    "\n",
    "Reportor = FindingDetails['Reportor'].tolist()\n",
    "Reportor = [\"-\" if pd.isnull(x) else x for x in Reportor]\n",
    "\n",
    "Contractor = FindingDetails['Contractor'].tolist()\n",
    "Contractor = [\"-\" if pd.isnull(x) else x for x in Contractor]\n",
    "\n",
    "Tof = FindingDetails['Type of finding'].tolist()\n",
    "Tof = [\"-\" if pd.isnull(x) else x for x in Tof]\n",
    "\n",
    "Topic = FindingDetails['Topic'].tolist()\n",
    "Topic = [\"-\" if pd.isnull(x) else x for x in Topic]\n",
    "\n",
    "Details = FindingDetails['Details'].tolist()\n",
    "Details = [\"-\" if pd.isnull(x) else x for x in Details]\n",
    "\n",
    "CorrectiveAction = FindingDetails['Corrective Action'].tolist()\n",
    "CorrectiveAction = [\"-\" if pd.isnull(x) else x for x in CorrectiveAction]\n",
    "\n",
    "IssueDate = FindingDetails['Issue Date'].tolist()\n",
    "IssueDate = [\"-\" if pd.isnull(x) else x for x in IssueDate]\n",
    "\n",
    "DueDate = FindingDetails['Due Date'].tolist()\n",
    "DueDate = [\"-\" if pd.isnull(x) else x for x in DueDate]\n",
    "\n",
    "Status = FindingDetails['Status'].tolist()\n",
    "Status = [\"-\" if pd.isnull(x) else x for x in Status]\n",
    "\n",
    "FindingData = list(zip(FindingNo, Company, Area, SubArea, \n",
    "                      Reportor, Contractor, Tof, \n",
    "                      Topic, Details, CorrectiveAction,\n",
    "                      IssueDate, DueDate, Status))\n",
    "\n",
    "if len(FindingData) > 0:\n",
    "    Query = \"INSERT INTO [FindingDetails] VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "\n",
    "    cursor.executemany(Query, FindingData)\n",
    "\n",
    "    connect_db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "Old_Size = len(FindingNo)\n",
    "\n",
    "Head = ['Source', 'Old', 'Latest']\n",
    "\n",
    "UpdateSize = ['FindingDetails', TotalOldFindingDetails, TotalOldFindingDetails+Old_Size]\n",
    "\n",
    "with open('./SMIT_Data/TotalData.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerow(UpdateSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepared_FindingNo = []\n",
    "Prepared_Area = []\n",
    "Prepared_SubArea = []\n",
    "Prepared_Contractor = []\n",
    "Prepared_Tof = []\n",
    "Prepared_Topic = []\n",
    "Prepared_Details = []\n",
    "Prepared_Frequency = []\n",
    "Prepared_CleansingDetails = []\n",
    "Prepared_Translate_Details = []\n",
    "Prepared_ListFindingNo = []\n",
    "MostMatch = []\n",
    "\n",
    "Encode_Translate_Cleansing_Safety_Audit = model.encode(TranslateDetails)\n",
    "\n",
    "Cosine_Sim = util.cos_sim(Encode_Translate_Cleansing_Safety_Audit, Encode_Translate_Cleansing_Safety_Audit)\n",
    "\n",
    "# Index_Frequency = []\n",
    "Size = len(Encode_Translate_Cleansing_Safety_Audit)\n",
    "\n",
    "Sum = 0\n",
    "for i in range(0, Size):\n",
    "  Index_Most_Frequency = 0\n",
    "  Count_Frequency = 0\n",
    "  Max_Cosine = 0\n",
    "  Index_Frequency = []\n",
    "  if i not in MostMatch:\n",
    "    # Index_Frequency.append(i)\n",
    "    if (Tof[i] == \"Unsafe Condition\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.6 and Tof[j] == \"Unsafe Condition\":\n",
    "              Count_Frequency += 1 \n",
    "              Index_Frequency.append(j) \n",
    "              if Cosine_Sim[i][j] > Max_Cosine:\n",
    "                Max_Cosine = Cosine_Sim[i][j]\n",
    "                Index_Most_Frequency = j\n",
    "        \n",
    "    elif (Tof[i] == \"Unsafe Action\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.6 and Tof[j] == \"Unsafe Action\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:\n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"HNM\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"HNM\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine: \n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"Near Miss\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"Near Miss\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:\n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "\n",
    "    elif (Tof[i] == \"Accident\"):\n",
    "      for j in range(0, Size):\n",
    "        if i != j:\n",
    "          if Cosine_Sim[i][j] > 0.7 and Tof[j] == \"Accident\": \n",
    "            Count_Frequency += 1 \n",
    "            Index_Frequency.append(j) \n",
    "            if Cosine_Sim[i][j] > Max_Cosine:   \n",
    "              Max_Cosine = Cosine_Sim[i][j]\n",
    "              Index_Most_Frequency = j\n",
    "              \n",
    "    if Count_Frequency == 0:\n",
    "      Index_Frequency.append(i)\n",
    "      Count_Frequency = 1 \n",
    "      Index_Most_Frequency = i  \n",
    "\n",
    "    if (Index_Most_Frequency not in MostMatch) and Details[Index_Most_Frequency] not in Prepared_Details:\n",
    "      Prepared_FindingNo.append(int(FindingNo[Index_Most_Frequency]))\n",
    "      Prepared_Area.append(Area[Index_Most_Frequency])\n",
    "      Prepared_SubArea.append(SubArea[Index_Most_Frequency])\n",
    "      Prepared_Contractor.append(Contractor[Index_Most_Frequency])\n",
    "      Prepared_Tof.append(Tof[Index_Most_Frequency])\n",
    "      Prepared_Topic.append(Topic[Index_Most_Frequency])\n",
    "      Prepared_Details.append(Details[Index_Most_Frequency])\n",
    "      Prepared_Frequency.append(Count_Frequency)\n",
    "      Prepared_CleansingDetails.append(CleansingDetails[Index_Most_Frequency])\n",
    "      Prepared_Translate_Details.append(TranslateDetails[Index_Most_Frequency])\n",
    "      Prepared_ListFindingNo.append(Index_Frequency)\n",
    "      MostMatch.append(Index_Most_Frequency)\n",
    "\n",
    "    if Index_Most_Frequency in MostMatch and (Details[Index_Most_Frequency] in Prepared_Details and Area[Index_Most_Frequency] in Prepared_Area):\n",
    "      Index = MostMatch.index(Index_Most_Frequency)\n",
    "      Temp_Prepared_ListFindingNo = Prepared_ListFindingNo[Index]+Index_Frequency\n",
    "      Temp_Prepared_ListFindingNo = list(set(Temp_Prepared_ListFindingNo))\n",
    "      Prepared_ListFindingNo[Index] = Temp_Prepared_ListFindingNo\n",
    "      Prepared_Frequency[Index] = len(Temp_Prepared_ListFindingNo)\n",
    "\n",
    "# print(MostMatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2423\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "result = len(set(list(chain.from_iterable(Prepared_ListFindingNo))))\n",
    "\n",
    "print(result)\n",
    "\n",
    "List = [832, 1028, 1348, 712, 936, 1161, 2028, 593, 915, 916, 1523, 633, 474, 636, 861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "1607\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "connect_db = pyodbc.connect(Driver = \"ODBC Driver 17 for SQL Server\",\n",
    "                            Server = \"TONY\",\n",
    "                            Database = \"SMIT3\",\n",
    "                            uid = 'Local_SMIT3.0',\n",
    "                            pwd = 'Tony123456',\n",
    "                            Trusted_Connection = 'yes')      \n",
    "\n",
    "print(len(Prepared_FindingNo))\n",
    "\n",
    "ResponsePreparedFindingDetail = pd.read_sql(\"SELECT * FROM Prepared_FindingDetails;\", connect_db)\n",
    "\n",
    "ResponsePreparedFindingDetail = list(ResponsePreparedFindingDetail.itertuples(index=False, name=None))\n",
    "\n",
    "print(len(ResponsePreparedFindingDetail))\n",
    "\n",
    "if(len(ResponsePreparedFindingDetail) > 0):\n",
    "\n",
    "    cursor = connect_db.cursor()\n",
    "    Query = \"DELETE FROM [Prepared_FindingDetails];\"\n",
    "    cursor.execute(Query)\n",
    "    ResponsePreparedFindingDetail = []\n",
    "\n",
    "Prepared_Safety_Audit = list(zip(Prepared_FindingNo, Prepared_Area, Prepared_SubArea, \n",
    "                            Prepared_Contractor, Prepared_Tof, Prepared_Topic, Prepared_Details, \n",
    "                            Prepared_Frequency, Prepared_CleansingDetails, Prepared_Translate_Details))\n",
    "\n",
    "cursor = connect_db.cursor()\n",
    "Query = \"INSERT INTO [Prepared_FindingDetails] VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\"\n",
    "\n",
    "cursor.executemany(Query, Prepared_Safety_Audit)\n",
    "connect_db.commit()\n",
    "\n",
    "Head = ['FindingNo', 'Area', 'SubArea', 'Contractor', 'TypeOfFinding', 'Topic', 'Details', 'Frequency', 'CleansingDetails', 'TranslateDetails']\n",
    "\n",
    "with open('./SMIT_Data/Prepared_Safety_Audit.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "  write = csv.writer(f)\n",
    "  write.writerow(Head)\n",
    "  write.writerows(Prepared_Safety_Audit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
